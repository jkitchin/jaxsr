{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Serialization & Sharing: Save, Load, Export, Reproduce\n",
    "\n",
    "JAXSR provides multiple serialization formats so you can:\n",
    "\n",
    "- **Save and load** models, basis libraries, and DOE studies\n",
    "- **Share** results with collaborators (even those without JAXSR)\n",
    "- **Deploy** models as pure NumPy callables (no JAX dependency)\n",
    "- **Reproduce** analyses from saved artifacts\n",
    "- **Export** equations to LaTeX, SymPy, or callable functions\n",
    "\n",
    "## What Can Be Serialized?\n",
    "\n",
    "| Object | Format | File Extension | Contains |\n",
    "|--------|--------|---------------|----------|\n",
    "| `SymbolicRegressor` | JSON | `.json` | Model expression, coefficients, metrics, basis config |\n",
    "| `BasisLibrary` | JSON | `.json` | All basis function definitions (except custom/parametric) |\n",
    "| `DOEStudy` | ZIP | `.jaxsr` | Study metadata, designs, observations, fitted model, history |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "from jaxsr import BasisLibrary, DOEStudy, SymbolicRegressor, fit_symbolic\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# We'll save everything to a temp directory\n",
    "SAVE_DIR = tempfile.mkdtemp(prefix=\"jaxsr_serial_\")\n",
    "print(f\"Save directory: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1",
   "metadata": {},
   "source": [
    "## 1. Saving and Loading Models\n",
    "\n",
    "A `SymbolicRegressor` can be saved to JSON and loaded back with full fidelity —\n",
    "the loaded model produces identical predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a model\n",
    "n = 80\n",
    "X = np.column_stack([\n",
    "    np.random.uniform(0, 5, n),\n",
    "    np.random.uniform(0, 5, n),\n",
    "])\n",
    "y = 2.5 * X[:, 0] + 1.2 * X[:, 0] * X[:, 1] - 0.8 * X[:, 1]**2 + 0.3 * np.random.randn(n)\n",
    "\n",
    "library = (\n",
    "    BasisLibrary(n_features=2, feature_names=[\"x1\", \"x2\"])\n",
    "    .add_constant()\n",
    "    .add_linear()\n",
    "    .add_polynomials(max_degree=3)\n",
    "    .add_interactions(max_order=2)\n",
    ")\n",
    "\n",
    "model = SymbolicRegressor(\n",
    "    basis_library=library, max_terms=5, information_criterion=\"bic\"\n",
    ")\n",
    "model.fit(jnp.array(X), jnp.array(y))\n",
    "print(f\"Fitted model: {model.expression_}\")\n",
    "print(f\"R² = {model.metrics_['r2']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_path = os.path.join(SAVE_DIR, \"model.json\")\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"File size: {os.path.getsize(model_path)} bytes\")\n",
    "\n",
    "# Load it back\n",
    "loaded_model = SymbolicRegressor.load(model_path)\n",
    "print(f\"\\nLoaded model: {loaded_model.expression_}\")\n",
    "\n",
    "# Verify predictions match\n",
    "X_test = np.random.uniform(0, 5, (10, 2))\n",
    "y_orig = model.predict(jnp.array(X_test))\n",
    "y_loaded = loaded_model.predict(jnp.array(X_test))\n",
    "max_diff = float(jnp.max(jnp.abs(y_orig - y_loaded)))\n",
    "print(f\"\\nMax prediction difference: {max_diff:.2e}\")\n",
    "assert max_diff < 1e-6, \"Predictions should match!\"\n",
    "print(\"Predictions match perfectly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect-json",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek inside the JSON file\n",
    "with open(model_path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"Model JSON structure:\")\n",
    "print(f\"  Top-level keys: {list(data.keys())}\")\n",
    "\n",
    "# The model JSON nests data under 'config', 'basis_library', 'result', etc.\n",
    "if \"result\" in data:\n",
    "    result_keys = list(data[\"result\"].keys())\n",
    "    print(f\"  result keys: {result_keys}\")\n",
    "    print(f\"  Selected names: {data['result'].get('selected_names', 'N/A')}\")\n",
    "    print(f\"  n_coefficients: {len(data['result'].get('coefficients', []))}\")\n",
    "if \"config\" in data:\n",
    "    print(f\"  config keys: {list(data['config'].keys())}\")\n",
    "if \"basis_library\" in data:\n",
    "    print(f\"  basis_library keys: {list(data['basis_library'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2",
   "metadata": {},
   "source": [
    "## 2. Saving and Loading Basis Libraries\n",
    "\n",
    "A `BasisLibrary` can be saved and loaded independently. This is useful when you\n",
    "want to reuse the same candidate function set across multiple datasets.\n",
    "\n",
    "**Limitation:** Custom functions (`add_custom`) and parametric functions\n",
    "(`add_parametric`) involve Python callables that cannot be serialized.\n",
    "They raise a `ValueError` on load and must be re-added manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-load-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a library\n",
    "library = (\n",
    "    BasisLibrary(n_features=3, feature_names=[\"T\", \"P\", \"flow\"])\n",
    "    .add_constant()\n",
    "    .add_linear()\n",
    "    .add_polynomials(max_degree=2)\n",
    "    .add_interactions(max_order=2)\n",
    "    .add_transcendental(funcs=[\"log\", \"exp\", \"sqrt\"])\n",
    ")\n",
    "print(f\"Library: {len(library)} basis functions\")\n",
    "\n",
    "# Save\n",
    "lib_path = os.path.join(SAVE_DIR, \"basis_library.json\")\n",
    "library.save(lib_path)\n",
    "print(f\"Saved to: {lib_path} ({os.path.getsize(lib_path)} bytes)\")\n",
    "\n",
    "# Load\n",
    "loaded_lib = BasisLibrary.load(lib_path)\n",
    "print(f\"\\nLoaded library: {len(loaded_lib)} basis functions\")\n",
    "print(f\"Names match: {library.names == loaded_lib.names}\")\n",
    "\n",
    "# Verify evaluation matches\n",
    "X_check = np.random.uniform(0.1, 5, (5, 3))\n",
    "Phi_orig = library.evaluate(jnp.array(X_check))\n",
    "Phi_loaded = loaded_lib.evaluate(jnp.array(X_check))\n",
    "max_diff = float(jnp.max(jnp.abs(Phi_orig - Phi_loaded)))\n",
    "print(f\"Max evaluation difference: {max_diff:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3",
   "metadata": {},
   "source": [
    "## 3. DOE Study Archives (`.jaxsr` files)\n",
    "\n",
    "A `DOEStudy` saves everything — metadata, factor definitions, experimental\n",
    "designs, all observations (with timestamps and notes), and the fitted model —\n",
    "into a single `.jaxsr` ZIP archive.\n",
    "\n",
    "This is the most portable format: share the `.jaxsr` file and your collaborator\n",
    "can pick up exactly where you left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "doe-study-save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a study with multiple rounds of data\n",
    "study = DOEStudy(\n",
    "    name=\"heat_exchanger\",\n",
    "    factor_names=[\"Re\", \"Pr\"],\n",
    "    bounds=[(5000, 50000), (0.7, 50)],\n",
    "    description=\"Nusselt number correlation for heat exchanger design\",\n",
    ")\n",
    "\n",
    "# Round 1: initial design\n",
    "X1 = study.create_design(method=\"latin_hypercube\", n_points=15, random_state=42)\n",
    "y1 = 0.023 * X1[:, 0]**0.8 * X1[:, 1]**0.4 + np.random.randn(15) * 2\n",
    "study.add_observations(X1, y1, notes=\"Round 1: initial screening\")\n",
    "\n",
    "# Fit\n",
    "model = study.fit(max_terms=5)\n",
    "print(f\"Model: {model.expression_}\")\n",
    "print(f\"R² = {model.metrics_['r2']:.4f}\")\n",
    "\n",
    "# Round 2: additional data\n",
    "X2 = np.random.uniform([5000, 0.7], [50000, 50], size=(5, 2))\n",
    "y2 = 0.023 * X2[:, 0]**0.8 * X2[:, 1]**0.4 + np.random.randn(5) * 2\n",
    "study.add_observations(X2, y2, notes=\"Round 2: gap-filling experiments\")\n",
    "model = study.fit(max_terms=5)\n",
    "\n",
    "# Save\n",
    "study_path = os.path.join(SAVE_DIR, \"heat_exchanger.jaxsr\")\n",
    "study.save(study_path)\n",
    "print(f\"\\nStudy saved to: {study_path}\")\n",
    "print(f\"File size: {os.path.getsize(study_path)} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "doe-study-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the study on a different machine (or in a new session)\n",
    "loaded_study = DOEStudy.load(study_path)\n",
    "\n",
    "print(loaded_study.summary())\n",
    "print(f\"\\nTotal observations: {loaded_study.n_observations}\")\n",
    "print(f\"Is fitted: {loaded_study.is_fitted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "doe-continue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A collaborator can continue the study: suggest, add data, refit\n",
    "loaded_study = DOEStudy.load(study_path)\n",
    "\n",
    "# Suggest next experiments\n",
    "next_pts = loaded_study.suggest_next(n_points=3, strategy=\"space_filling\")\n",
    "print(\"Collaborator's next experiments:\")\n",
    "for i, pt in enumerate(next_pts):\n",
    "    print(f\"  Run {i+1}: Re = {pt[0]:.0f}, Pr = {pt[1]:.2f}\")\n",
    "\n",
    "# They run the experiments and add data\n",
    "y_new = 0.023 * next_pts[:, 0]**0.8 * next_pts[:, 1]**0.4 + np.random.randn(3) * 2\n",
    "loaded_study.add_observations(next_pts, y_new, notes=\"Collaborator round\")\n",
    "loaded_study.fit(max_terms=5)\n",
    "loaded_study.save(study_path)  # overwrite with updated study\n",
    "\n",
    "print(f\"\\nUpdated study: {loaded_study.n_observations} total observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4",
   "metadata": {},
   "source": [
    "## 4. Exporting Models for Deployment\n",
    "\n",
    "Once you have a good model, you may want to use it in production without\n",
    "requiring JAX or JAXSR as dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-callable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit our original model\n",
    "X = np.column_stack([\n",
    "    np.random.uniform(0, 5, 80),\n",
    "    np.random.uniform(0, 5, 80),\n",
    "])\n",
    "y = 2.5 * X[:, 0] - 0.8 * X[:, 1]**2 + 0.3 * np.random.randn(80)\n",
    "model = fit_symbolic(jnp.array(X), jnp.array(y), feature_names=[\"x1\", \"x2\"], max_terms=4)\n",
    "\n",
    "print(f\"Model: {model.expression_}\")\n",
    "print()\n",
    "\n",
    "# --- Export 1: Pure NumPy callable ---\n",
    "predict_fn = model.to_callable()\n",
    "\n",
    "# This function uses only NumPy — no JAX needed\n",
    "X_new = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "y_pred = predict_fn(X_new)\n",
    "print(f\"NumPy callable predictions: {y_pred}\")\n",
    "print(f\"Type: {type(predict_fn)}\")\n",
    "print()\n",
    "\n",
    "# --- Export 2: SymPy expression ---\n",
    "sympy_expr = model.to_sympy()\n",
    "print(f\"SymPy: {sympy_expr}\")\n",
    "print(f\"Type: {type(sympy_expr)}\")\n",
    "print()\n",
    "\n",
    "# SymPy enables symbolic manipulation\n",
    "import sympy\n",
    "\n",
    "x1, x2 = sympy.symbols(\"x1 x2\")\n",
    "derivative = sympy.diff(sympy_expr, x1)\n",
    "print(f\"d/dx1: {derivative}\")\n",
    "print()\n",
    "\n",
    "# --- Export 3: LaTeX ---\n",
    "latex = model.to_latex()\n",
    "print(f\"LaTeX: ${latex}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deploy-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deployment pattern: save the callable as a standalone module\n",
    "# This is how you'd integrate into a production system\n",
    "\n",
    "deploy_code = f'''\\\n",
    "\"\"\"Auto-generated prediction function from JAXSR.\n",
    "\n",
    "Model: {model.expression_}\n",
    "R² = {model.metrics_[\"r2\"]:.6f}\n",
    "Generated with JAXSR v0.1.0\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# Coefficients: {dict(zip(model.selected_features_, [float(c) for c in model.coefficients_]))}\n",
    "\n",
    "def predict(X):\n",
    "    \"\"\"Predict y from X array of shape (n_samples, 2).\n",
    "\n",
    "    Features: x1 (column 0), x2 (column 1)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(1, -1)\n",
    "    # Use the exported callable\n",
    "    from jaxsr import SymbolicRegressor\n",
    "    model = SymbolicRegressor.load(\"{model_path}\")\n",
    "    return np.asarray(model.predict(X))\n",
    "'''\n",
    "\n",
    "print(\"Example deployment module:\")\n",
    "print(deploy_code[:300] + \"...\")\n",
    "print()\n",
    "print(\"For zero-dependency deployment, use model.to_callable() directly.\")\n",
    "print(\"The callable is a pure Python/NumPy function with no JAX dependency.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5",
   "metadata": {},
   "source": [
    "## 5. Reproducibility Checklist\n",
    "\n",
    "To fully reproduce a JAXSR analysis, save these artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reproducibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete reproducibility: save everything\n",
    "repro_dir = os.path.join(SAVE_DIR, \"reproducible\")\n",
    "os.makedirs(repro_dir, exist_ok=True)\n",
    "\n",
    "# 1. Raw data\n",
    "np.savez(\n",
    "    os.path.join(repro_dir, \"data.npz\"),\n",
    "    X=X, y=y,\n",
    ")\n",
    "print(\"[1] Saved raw data → data.npz\")\n",
    "\n",
    "# 2. Basis library configuration\n",
    "library = (\n",
    "    BasisLibrary(n_features=2, feature_names=[\"x1\", \"x2\"])\n",
    "    .add_constant()\n",
    "    .add_linear()\n",
    "    .add_polynomials(max_degree=3)\n",
    "    .add_interactions(max_order=2)\n",
    ")\n",
    "library.save(os.path.join(repro_dir, \"library.json\"))\n",
    "print(\"[2] Saved basis library → library.json\")\n",
    "\n",
    "# 3. Fitted model\n",
    "model.save(os.path.join(repro_dir, \"model.json\"))\n",
    "print(\"[3] Saved fitted model → model.json\")\n",
    "\n",
    "# 4. Metadata\n",
    "metadata = {\n",
    "    \"jaxsr_version\": \"0.1.0\",\n",
    "    \"random_seed\": 42,\n",
    "    \"strategy\": \"greedy_forward\",\n",
    "    \"information_criterion\": \"bic\",\n",
    "    \"max_terms\": 4,\n",
    "    \"expression\": model.expression_,\n",
    "    \"metrics\": {k: float(v) for k, v in model.metrics_.items()},\n",
    "}\n",
    "with open(os.path.join(repro_dir, \"metadata.json\"), \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(\"[4] Saved metadata → metadata.json\")\n",
    "\n",
    "print(f\"\\nReproducibility package: {repro_dir}/\")\n",
    "for fname in sorted(os.listdir(repro_dir)):\n",
    "    size = os.path.getsize(os.path.join(repro_dir, fname))\n",
    "    print(f\"  {fname:25s}  {size:>8} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reproduce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce the analysis from saved artifacts\n",
    "print(\"Reproducing analysis from saved artifacts...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load data\n",
    "data = np.load(os.path.join(repro_dir, \"data.npz\"))\n",
    "X_repro, y_repro = data[\"X\"], data[\"y\"]\n",
    "\n",
    "# Load library and refit\n",
    "lib_repro = BasisLibrary.load(os.path.join(repro_dir, \"library.json\"))\n",
    "model_repro = SymbolicRegressor(\n",
    "    basis_library=lib_repro, max_terms=4, information_criterion=\"bic\"\n",
    ")\n",
    "model_repro.fit(jnp.array(X_repro), jnp.array(y_repro))\n",
    "\n",
    "# Or just load the saved model directly\n",
    "model_direct = SymbolicRegressor.load(os.path.join(repro_dir, \"model.json\"))\n",
    "\n",
    "print(f\"Refit model:  {model_repro.expression_}\")\n",
    "print(f\"Loaded model: {model_direct.expression_}\")\n",
    "print(f\"\\nExpressions match: {model_repro.expression_ == model_direct.expression_}\")\n",
    "\n",
    "# Verify predictions\n",
    "X_test = np.random.uniform(0, 5, (5, 2))\n",
    "y1 = model_repro.predict(jnp.array(X_test))\n",
    "y2 = model_direct.predict(jnp.array(X_test))\n",
    "print(f\"Max prediction difference: {float(jnp.max(jnp.abs(y1 - y2))):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6",
   "metadata": {},
   "source": [
    "## 6. What Cannot Be Serialized\n",
    "\n",
    "Some objects involve Python callables that cannot be saved to JSON:\n",
    "\n",
    "| Feature | Serializable? | Workaround |\n",
    "|---------|--------------|------------|\n",
    "| Standard basis (poly, transcendental) | Yes | Reconstructed from config |\n",
    "| Custom basis (`add_custom`) | **No** | Re-add manually after loading |\n",
    "| Parametric basis (`add_parametric`) | **No** | Re-add manually after loading |\n",
    "| Constraints | **No** | Reconstruct from code |\n",
    "| Fitted coefficients & metrics | Yes | Stored in model JSON |\n",
    "| Pareto front | Yes | Stored in model JSON |\n",
    "\n",
    "For parametric models (like the Langmuir example), save the model JSON\n",
    "for predictions, but keep the fitting script for full reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary files\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(SAVE_DIR)\n",
    "print(f\"Cleaned up: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Task | Method |\n",
    "|------|--------|\n",
    "| Save a model | `model.save(\"model.json\")` |\n",
    "| Load a model | `SymbolicRegressor.load(\"model.json\")` |\n",
    "| Save a basis library | `library.save(\"lib.json\")` |\n",
    "| Load a basis library | `BasisLibrary.load(\"lib.json\")` |\n",
    "| Save a DOE study | `study.save(\"study.jaxsr\")` |\n",
    "| Load a DOE study | `DOEStudy.load(\"study.jaxsr\")` |\n",
    "| Deploy without JAX | `predict_fn = model.to_callable()` |\n",
    "| Export to SymPy | `expr = model.to_sympy()` |\n",
    "| Export to LaTeX | `latex = model.to_latex()` |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **For DOE workflows**, use `.jaxsr` files — they capture the entire study lifecycle\n",
    "2. **For sharing models**, use `model.save()` — recipients can predict without refitting\n",
    "3. **For deployment**, use `model.to_callable()` — zero-dependency NumPy function\n",
    "4. **For reproducibility**, save data + library + model + metadata together\n",
    "5. **For publications**, use `model.to_latex()` and `model.to_sympy()`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}