{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Langmuir Isotherm: DOE-Driven Active Learning\n",
    "\n",
    "This notebook demonstrates a complete **Design of Experiments (DOE)** workflow with\n",
    "**active learning** to efficiently estimate the parameters of a known model — the\n",
    "Langmuir adsorption isotherm:\n",
    "\n",
    "$$q = \\frac{q_{\\max} \\, K \\, P}{1 + K \\, P}$$\n",
    "\n",
    "where:\n",
    "- $q$ — amount adsorbed (mol/g)\n",
    "- $q_{\\max}$ — monolayer saturation capacity\n",
    "- $K$ — equilibrium adsorption constant (1/bar)\n",
    "- $P$ — partial pressure (bar)\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. **Initial Design** — Generate a small Latin Hypercube design\n",
    "2. **Simulate Experiments** — Measure `q` at designed pressures (replace with real lab data)\n",
    "3. **Fit Langmuir Model** — Using parametric basis functions\n",
    "4. **Evaluate & Diagnose** — Residuals, parity plot, coefficient intervals\n",
    "5. **Active Learning Loop** — Suggest the most informative next experiments, collect data, refit\n",
    "6. **Final ANOVA** — Decompose variance to confirm model adequacy\n",
    "7. **Report** — Extract parameters, uncertainty, and LaTeX equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from jaxsr import (\n",
    "    AdaptiveSampler,\n",
    "    BasisLibrary,\n",
    "    DOEStudy,\n",
    "    SymbolicRegressor,\n",
    "    anova,\n",
    "    bootstrap_coefficients,\n",
    "    bootstrap_predict,\n",
    ")\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "true-model-header",
   "metadata": {},
   "source": [
    "## Ground Truth (for simulation)\n",
    "\n",
    "In a real study you would not know these values — they are what you are trying to\n",
    "discover. Here we define them so we can simulate \"running experiments\" and verify\n",
    "that the method recovers the true parameters.\n",
    "\n",
    "| Parameter | True Value | Units |\n",
    "|-----------|-----------|-------|\n",
    "| $q_{\\max}$ | 5.0 | mol/g |\n",
    "| $K$ | 2.0 | 1/bar |\n",
    "| Noise $\\sigma$ | 0.15 | mol/g |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "true-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Langmuir parameters (unknown to the experimenter)\n",
    "Q_MAX_TRUE = 5.0   # mol/g\n",
    "K_TRUE = 2.0       # 1/bar\n",
    "NOISE_STD = 0.15   # measurement noise\n",
    "\n",
    "# Pressure range for the study\n",
    "P_BOUNDS = [(0.01, 10.0)]  # bar\n",
    "\n",
    "\n",
    "def run_experiment(P_values):\n",
    "    \"\"\"Simulate running adsorption experiments.\n",
    "\n",
    "    Replace this function with actual lab measurements in a real study.\n",
    "    \"\"\"\n",
    "    P = np.asarray(P_values).flatten()\n",
    "    q_true = Q_MAX_TRUE * K_TRUE * P / (1 + K_TRUE * P)\n",
    "    q_measured = q_true + NOISE_STD * np.random.randn(len(P))\n",
    "    return q_measured\n",
    "\n",
    "\n",
    "# Visualize the true isotherm\n",
    "P_dense = np.linspace(0.01, 10.0, 200)\n",
    "q_dense = Q_MAX_TRUE * K_TRUE * P_dense / (1 + K_TRUE * P_dense)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(P_dense, q_dense, \"k-\", linewidth=2, label=\"True Langmuir\")\n",
    "ax.axhline(Q_MAX_TRUE, color=\"gray\", linestyle=\"--\", alpha=0.5, label=f\"$q_{{max}}$ = {Q_MAX_TRUE}\")\n",
    "ax.axvline(1.0 / K_TRUE, color=\"gray\", linestyle=\":\", alpha=0.5, label=f\"$P_{{1/2}}$ = 1/K = {1/K_TRUE}\")\n",
    "ax.set_xlabel(\"Pressure (bar)\")\n",
    "ax.set_ylabel(\"Amount adsorbed (mol/g)\")\n",
    "ax.set_title(\"Langmuir Isotherm — True Model\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-header",
   "metadata": {},
   "source": [
    "## Step 1: Initial Experimental Design\n",
    "\n",
    "We start with a small Latin Hypercube design of **8 points**. This is deliberately\n",
    "small — active learning will tell us where to measure next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-design",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DOE study\n",
    "study = DOEStudy(\n",
    "    name=\"langmuir_isotherm\",\n",
    "    factor_names=[\"pressure\"],\n",
    "    bounds=P_BOUNDS,\n",
    "    description=\"Langmuir isotherm parameter estimation via active learning\",\n",
    ")\n",
    "\n",
    "# Generate initial design: 8 points\n",
    "X_init = study.create_design(method=\"latin_hypercube\", n_points=8, random_state=42)\n",
    "\n",
    "# Run the initial experiments\n",
    "y_init = run_experiment(X_init)\n",
    "study.add_observations(X_init, y_init, notes=\"Round 0: initial LHS design (8 points)\")\n",
    "\n",
    "print(f\"Initial design: {len(X_init)} points\")\n",
    "print(f\"Pressure range: [{X_init.min():.3f}, {X_init.max():.3f}] bar\")\n",
    "print(f\"Response range: [{y_init.min():.3f}, {y_init.max():.3f}] mol/g\")\n",
    "print()\n",
    "print(\"Design points:\")\n",
    "for i in range(len(X_init)):\n",
    "    print(f\"  P = {X_init[i, 0]:6.3f} bar  →  q = {y_init[i]:.3f} mol/g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-header",
   "metadata": {},
   "source": [
    "## Step 2: Build the Langmuir Basis Library\n",
    "\n",
    "The Langmuir model $q = q_{\\max} \\cdot \\frac{KP}{1+KP}$ is:\n",
    "- **Linear** in $q_{\\max}$ (it's a coefficient)\n",
    "- **Nonlinear** in $K$ (it appears inside the function)\n",
    "\n",
    "We encode this using `add_parametric`: JAXSR optimizes $K$ by profile likelihood,\n",
    "and estimates $q_{\\max}$ as the linear coefficient via OLS.\n",
    "\n",
    "We also add a constant term to check if the data has an offset (a perfect Langmuir\n",
    "model should have zero intercept)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basis-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "library = (\n",
    "    BasisLibrary(n_features=1, feature_names=[\"P\"])\n",
    "    .add_constant()  # Intercept — should be ~0 if Langmuir is correct\n",
    "    .add_parametric(\n",
    "        name=\"K*P/(1+K*P)\",\n",
    "        func=lambda X, K: K * X[:, 0] / (1 + K * X[:, 0]),\n",
    "        param_bounds={\"K\": (0.01, 100.0)},\n",
    "        complexity=3,\n",
    "        feature_indices=(0,),\n",
    "        log_scale=True,  # K spans orders of magnitude\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Basis library: {len(library)} candidate functions\")\n",
    "for i, name in enumerate(library.names):\n",
    "    print(f\"  [{i}] {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-header",
   "metadata": {},
   "source": [
    "## Step 3: Initial Model Fit\n",
    "\n",
    "Fit the Langmuir model to the initial 8 data points. With so few points,\n",
    "we use AICc (corrected AIC) which penalizes overfitting more than AIC or BIC\n",
    "when sample size is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-fit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_langmuir(X, y):\n",
    "    \"\"\"Fit the Langmuir model to data.\"\"\"\n",
    "    model = SymbolicRegressor(\n",
    "        basis_library=library,\n",
    "        max_terms=2,\n",
    "        strategy=\"greedy_forward\",\n",
    "        information_criterion=\"aicc\",\n",
    "    )\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = fit_langmuir(X_init, y_init)\n",
    "\n",
    "print(model.summary())\n",
    "print(f\"\\nExpression: {model.expression_}\")\n",
    "print(f\"R²: {model.metrics_['r2']:.6f}\")\n",
    "print(f\"MSE: {model.metrics_['mse']:.6g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-diagnostics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic plots for initial fit\n",
    "P_plot = np.linspace(0.01, 10.0, 200).reshape(-1, 1)\n",
    "y_pred_curve = model.predict(jnp.array(P_plot))\n",
    "y_pred_train = model.predict(jnp.array(X_init))\n",
    "residuals = np.asarray(y_init) - np.asarray(y_pred_train)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4.5))\n",
    "\n",
    "# 1. Isotherm fit\n",
    "axes[0].plot(P_dense, q_dense, \"k--\", alpha=0.5, label=\"True\")\n",
    "axes[0].plot(P_plot, y_pred_curve, \"b-\", linewidth=2, label=\"Fitted\")\n",
    "axes[0].scatter(X_init[:, 0], y_init, color=\"red\", s=60, zorder=5, label=\"Data (n=8)\")\n",
    "axes[0].set_xlabel(\"Pressure (bar)\")\n",
    "axes[0].set_ylabel(\"q (mol/g)\")\n",
    "axes[0].set_title(\"Initial Fit (8 points)\")\n",
    "axes[0].legend()\n",
    "\n",
    "# 2. Parity plot\n",
    "axes[1].scatter(y_init, y_pred_train, color=\"blue\", s=60)\n",
    "lims = [min(y_init.min(), float(y_pred_train.min())), max(y_init.max(), float(y_pred_train.max()))]\n",
    "axes[1].plot(lims, lims, \"r--\", alpha=0.5)\n",
    "axes[1].set_xlabel(\"Measured\")\n",
    "axes[1].set_ylabel(\"Predicted\")\n",
    "axes[1].set_title(f\"Parity (R² = {model.metrics_['r2']:.4f})\")\n",
    "axes[1].set_aspect(\"equal\")\n",
    "\n",
    "# 3. Residuals\n",
    "axes[2].scatter(X_init[:, 0], residuals, color=\"blue\", s=60)\n",
    "axes[2].axhline(y=0, color=\"r\", linestyle=\"--\", alpha=0.5)\n",
    "axes[2].set_xlabel(\"Pressure (bar)\")\n",
    "axes[2].set_ylabel(\"Residual (mol/g)\")\n",
    "axes[2].set_title(f\"Residuals (std = {residuals.std():.4f})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-header",
   "metadata": {},
   "source": [
    "## Step 4: Active Learning Loop\n",
    "\n",
    "Now we iteratively:\n",
    "1. Use the current model to identify where uncertainty is highest\n",
    "2. Suggest 3 new experiments at those pressures\n",
    "3. \"Run\" the experiments (simulate)\n",
    "4. Refit the model with all accumulated data\n",
    "\n",
    "We run **4 rounds**, adding 3 points each time (8 → 11 → 14 → 17 → 20 total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-learning-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROUNDS = 4\n",
    "N_PER_ROUND = 3\n",
    "\n",
    "# Track history for plotting\n",
    "X_all = X_init.copy()\n",
    "y_all = y_init.copy()\n",
    "history = [{\n",
    "    \"round\": 0,\n",
    "    \"n_points\": len(y_init),\n",
    "    \"r2\": model.metrics_[\"r2\"],\n",
    "    \"mse\": model.metrics_[\"mse\"],\n",
    "    \"expression\": model.expression_,\n",
    "}]\n",
    "\n",
    "print(f\"Round 0: {len(y_all):2d} points | R² = {model.metrics_['r2']:.6f} | \"\n",
    "      f\"MSE = {model.metrics_['mse']:.6g}\")\n",
    "print(f\"  Model: {model.expression_}\")\n",
    "print()\n",
    "\n",
    "for round_num in range(1, N_ROUNDS + 1):\n",
    "    # 1. Suggest next experiments using uncertainty-based active learning\n",
    "    sampler = AdaptiveSampler(\n",
    "        model=model,\n",
    "        bounds=P_BOUNDS,\n",
    "        strategy=\"uncertainty\",\n",
    "        batch_size=N_PER_ROUND,\n",
    "        n_candidates=500,\n",
    "        random_state=round_num * 10,\n",
    "    )\n",
    "    result = sampler.suggest(\n",
    "        n_points=N_PER_ROUND,\n",
    "        exclude_points=X_all,  # Don't repeat existing measurements\n",
    "        min_distance=0.05,\n",
    "    )\n",
    "    X_new = np.array(result.points)\n",
    "\n",
    "    # 2. Run experiments at suggested pressures\n",
    "    y_new = run_experiment(X_new)\n",
    "\n",
    "    # 3. Accumulate data\n",
    "    X_all = np.vstack([X_all, X_new])\n",
    "    y_all = np.concatenate([y_all, y_new])\n",
    "\n",
    "    # 4. Refit model with all data\n",
    "    model = fit_langmuir(X_all, y_all)\n",
    "\n",
    "    # Log\n",
    "    history.append({\n",
    "        \"round\": round_num,\n",
    "        \"n_points\": len(y_all),\n",
    "        \"r2\": model.metrics_[\"r2\"],\n",
    "        \"mse\": model.metrics_[\"mse\"],\n",
    "        \"expression\": model.expression_,\n",
    "    })\n",
    "\n",
    "    suggested_P = \", \".join(f\"{p[0]:.3f}\" for p in X_new)\n",
    "    print(f\"Round {round_num}: {len(y_all):2d} points | R² = {model.metrics_['r2']:.6f} | \"\n",
    "          f\"MSE = {model.metrics_['mse']:.6g}\")\n",
    "    print(f\"  Suggested P = [{suggested_P}]\")\n",
    "    print(f\"  Model: {model.expression_}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learning-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "rounds = [h[\"round\"] for h in history]\n",
    "n_pts = [h[\"n_points\"] for h in history]\n",
    "r2s = [h[\"r2\"] for h in history]\n",
    "mses = [h[\"mse\"] for h in history]\n",
    "\n",
    "# R² vs data size\n",
    "axes[0].plot(n_pts, r2s, \"bo-\", linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel(\"Number of data points\")\n",
    "axes[0].set_ylabel(\"R²\")\n",
    "axes[0].set_title(\"Model Accuracy vs Data Size\")\n",
    "axes[0].set_ylim(bottom=min(r2s) - 0.01)\n",
    "for i, (n, r2) in enumerate(zip(n_pts, r2s, strict=False)):\n",
    "    axes[0].annotate(f\"Round {i}\", (n, r2), textcoords=\"offset points\",\n",
    "                     xytext=(8, -8), fontsize=9)\n",
    "\n",
    "# MSE vs data size\n",
    "axes[1].plot(n_pts, mses, \"ro-\", linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel(\"Number of data points\")\n",
    "axes[1].set_ylabel(\"MSE\")\n",
    "axes[1].set_title(\"MSE vs Data Size\")\n",
    "axes[1].set_yscale(\"log\")\n",
    "for i, (n, mse) in enumerate(zip(n_pts, mses, strict=False)):\n",
    "    axes[1].annotate(f\"Round {i}\", (n, mse), textcoords=\"offset points\",\n",
    "                     xytext=(8, 5), fontsize=9)\n",
    "\n",
    "plt.suptitle(\"Active Learning Progress\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "all-data-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all data and final fit\n",
    "P_plot = np.linspace(0.01, 10.0, 200).reshape(-1, 1)\n",
    "y_final_curve = model.predict(jnp.array(P_plot))\n",
    "y_pred_pi, pi_lo, pi_hi = model.predict_interval(jnp.array(P_plot), alpha=0.05)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# True curve\n",
    "ax.plot(P_dense, q_dense, \"k--\", alpha=0.4, linewidth=1.5, label=\"True Langmuir\")\n",
    "\n",
    "# Prediction interval\n",
    "ax.fill_between(\n",
    "    P_plot.flatten(),\n",
    "    np.asarray(pi_lo),\n",
    "    np.asarray(pi_hi),\n",
    "    alpha=0.15, color=\"blue\", label=\"95% prediction interval\",\n",
    ")\n",
    "\n",
    "# Fitted curve\n",
    "ax.plot(P_plot, y_final_curve, \"b-\", linewidth=2, label=\"Fitted model\")\n",
    "\n",
    "# Data points colored by round\n",
    "colors = plt.cm.viridis(np.linspace(0, 0.9, N_ROUNDS + 1))\n",
    "offset = 0\n",
    "for r in range(N_ROUNDS + 1):\n",
    "    n = 8 if r == 0 else N_PER_ROUND\n",
    "    ax.scatter(\n",
    "        X_all[offset:offset + n, 0], y_all[offset:offset + n],\n",
    "        color=colors[r], s=70, edgecolors=\"black\", linewidth=0.5,\n",
    "        zorder=5, label=f\"Round {r} ({n} pts)\",\n",
    "    )\n",
    "    offset += n\n",
    "\n",
    "ax.set_xlabel(\"Pressure (bar)\", fontsize=12)\n",
    "ax.set_ylabel(\"Amount adsorbed (mol/g)\", fontsize=12)\n",
    "ax.set_title(\"Langmuir Isotherm — Final Fit with Active Learning\", fontsize=13)\n",
    "ax.legend(loc=\"lower right\", fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-header",
   "metadata": {},
   "source": [
    "## Step 5: Parameter Extraction & Uncertainty\n",
    "\n",
    "The fitted model gives us:\n",
    "- **$q_{\\max}$** — the coefficient of the Langmuir term\n",
    "- **$K$** — the optimized nonlinear parameter inside the basis function\n",
    "\n",
    "We use both OLS intervals (for $q_{\\max}$) and bootstrap (for both parameters\n",
    "including the nonlinear $K$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parameter-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PARAMETER ESTIMATES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nFinal model: {model.expression_}\")\n",
    "print(f\"Total data points: {len(y_all)}\")\n",
    "print(f\"R²: {model.metrics_['r2']:.6f}\")\n",
    "print(f\"MSE: {model.metrics_['mse']:.6g}\")\n",
    "print(f\"Estimated noise std (sigma): {model.sigma_:.4f}  (true: {NOISE_STD})\")\n",
    "\n",
    "# Coefficients\n",
    "print(\"\\nCoefficients:\")\n",
    "for name, coef in zip(model.selected_features_, model.coefficients_, strict=False):\n",
    "    print(f\"  {name}: {float(coef):.4f}\")\n",
    "\n",
    "# The coefficient of the Langmuir term IS q_max\n",
    "langmuir_idx = [i for i, n in enumerate(model.selected_features_) if \"K\" in n or \"/\" in n]\n",
    "if langmuir_idx:\n",
    "    q_max_est = float(model.coefficients_[langmuir_idx[0]])\n",
    "    print(f\"\\n  → q_max estimate: {q_max_est:.4f}  (true: {Q_MAX_TRUE})\")\n",
    "    print(f\"    Error: {abs(q_max_est - Q_MAX_TRUE):.4f} ({abs(q_max_est - Q_MAX_TRUE)/Q_MAX_TRUE*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ols-intervals",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS confidence intervals for q_max (the linear coefficient)\n",
    "print(\"95% OLS Coefficient Confidence Intervals:\")\n",
    "print(\"-\" * 50)\n",
    "intervals = model.coefficient_intervals(alpha=0.05)\n",
    "for name, (lo, hi) in intervals.items():\n",
    "    sig = \" ***\" if lo * hi > 0 else \"  (not significant)\"\n",
    "    print(f\"  {name:30s}: [{lo:8.4f}, {hi:8.4f}]{sig}\")\n",
    "\n",
    "print(\"\\nNote: A significant intercept suggests the Langmuir model\")\n",
    "print(\"may not perfectly describe the data (offset present).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bootstrap-uq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap — captures uncertainty in BOTH q_max and K\n",
    "print(\"Bootstrap Analysis (1000 resamples):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "boot_coef = bootstrap_coefficients(model, n_bootstrap=1000, alpha=0.05, seed=42)\n",
    "\n",
    "print(\"\\n95% Bootstrap Coefficient Intervals:\")\n",
    "for name, (lo, hi) in boot_coef.intervals.items():\n",
    "    mean_val = boot_coef.means[name]\n",
    "    print(f\"  {name:30s}: {mean_val:8.4f}  [{lo:8.4f}, {hi:8.4f}]\")\n",
    "\n",
    "# Bootstrap prediction intervals on a fine grid\n",
    "P_pred = np.linspace(0.01, 10.0, 50).reshape(-1, 1)\n",
    "boot_pred = bootstrap_predict(model, jnp.array(P_pred), n_bootstrap=1000, alpha=0.05, seed=42)\n",
    "\n",
    "avg_width = np.mean(np.asarray(boot_pred.upper) - np.asarray(boot_pred.lower))\n",
    "print(f\"\\nAverage 95% bootstrap prediction interval width: {avg_width:.4f} mol/g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conformal-uq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conformal prediction — distribution-free intervals\n",
    "P_test = np.linspace(0.5, 9.5, 30).reshape(-1, 1)\n",
    "y_conf, conf_lo, conf_hi = model.predict_conformal(\n",
    "    jnp.array(P_test), alpha=0.05, method=\"jackknife+\"\n",
    ")\n",
    "\n",
    "conf_width = np.mean(np.asarray(conf_hi) - np.asarray(conf_lo))\n",
    "print(f\"Conformal (jackknife+) 95% interval:\")\n",
    "print(f\"  Average width: {conf_width:.4f} mol/g\")\n",
    "print(f\"\\nSample intervals:\")\n",
    "print(f\"  {'P (bar)':>8}  {'Predicted':>10}  {'Lower':>8}  {'Upper':>8}\")\n",
    "for i in range(0, len(P_test), 6):\n",
    "    print(f\"  {P_test[i, 0]:8.2f}  {float(y_conf[i]):10.4f}  \"\n",
    "          f\"{float(conf_lo[i]):8.4f}  {float(conf_hi[i]):8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6-header",
   "metadata": {},
   "source": [
    "## Step 6: ANOVA — Model Adequacy\n",
    "\n",
    "ANOVA decomposes the total sum of squares into contributions from each model term.\n",
    "For a well-specified Langmuir model:\n",
    "\n",
    "- The **Langmuir term** should explain > 95% of the variance\n",
    "- The **constant (intercept)** should contribute very little (< 5%)\n",
    "- If the intercept contributes significantly, the model may have a systematic offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anova",
   "metadata": {},
   "outputs": [],
   "source": "# ANOVA decomposition\nanova_result = anova(model)\n\n# Get total SS for computing percent contributions\nsummary_sources = {\"Model\", \"Residual\", \"Total\"}\ntotal_row = [r for r in anova_result.rows if r.source == \"Total\"]\ntotal_ss = total_row[0].sum_sq if total_row else sum(\n    r.sum_sq for r in anova_result.rows if r.source not in summary_sources\n)\n\nprint(\"ANOVA Decomposition\")\nprint(\"=\" * 80)\nprint(f\"  {'Source':25s}  {'DF':>4}  {'Sum Sq':>12}  {'Mean Sq':>12}  {'F':>10}  {'p-value':>10}\")\nprint(\"-\" * 80)\nfor row in anova_result.rows:\n    f_str = f\"{row.f_value:10.2f}\" if row.f_value is not None else \"          \"\n    p_str = f\"{row.p_value:10.4f}\" if row.p_value is not None else \"          \"\n    print(f\"  {row.source:25s}  {row.df:4d}  {row.sum_sq:12.4f}  {row.mean_sq:12.4f}  {f_str}  {p_str}\")\nprint(\"-\" * 80)\n\n# Compute and display percent contributions for model terms\nprint(\"\\nVariance Contributions (model terms only):\")\nterm_rows = [r for r in anova_result.rows if r.source not in summary_sources]\nmodel_ss = sum(r.sum_sq for r in term_rows)\nfor row in term_rows:\n    pct = 100 * row.sum_sq / model_ss if model_ss > 0 else 0\n    bar = \"█\" * int(pct / 2)\n    sig = \"***\" if row.p_value is not None and row.p_value < 0.001 else (\n        \"**\" if row.p_value is not None and row.p_value < 0.01 else (\n        \"*\" if row.p_value is not None and row.p_value < 0.05 else \"\"))\n    print(f\"  {row.source:25s}  {pct:6.1f}%  {bar}  {sig}\")\n\n# Interpretation\nlangmuir_rows = [r for r in term_rows if \"/\" in r.source or \"K\" in r.source]\nconst_rows = [r for r in term_rows if r.source.strip() == \"1\"]\n\nprint(\"\\nInterpretation:\")\nif langmuir_rows:\n    lang_pct = 100 * langmuir_rows[0].sum_sq / model_ss if model_ss > 0 else 0\n    if lang_pct > 95:\n        print(f\"  Langmuir term explains {lang_pct:.1f}% of model SS — model is well-specified.\")\n    else:\n        print(f\"  Langmuir term explains {lang_pct:.1f}% — consider model refinement.\")\n\nif const_rows:\n    const_pct = 100 * const_rows[0].sum_sq / model_ss if model_ss > 0 else 0\n    if const_pct > 5:\n        print(f\"  Intercept contributes {const_pct:.1f}% — check for systematic offset.\")\n    else:\n        print(f\"  Intercept is negligible ({const_pct:.1f}%) — no systematic offset.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anova-viz",
   "metadata": {},
   "outputs": [],
   "source": "# ANOVA visualization — percent contribution by term\nsummary_sources = {\"Model\", \"Residual\", \"Total\"}\nterm_rows = [r for r in anova_result.rows if r.source not in summary_sources]\nmodel_ss = sum(r.sum_sq for r in term_rows)\n\nsources = [r.source for r in term_rows]\ncontributions = [100 * r.sum_sq / model_ss if model_ss > 0 else 0 for r in term_rows]\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Pie chart of variance contributions\ncolors_pie = [\"#2196F3\", \"#FF9800\", \"#4CAF50\", \"#F44336\"][:len(sources)]\n\naxes[0].pie(\n    contributions, labels=sources, autopct=\"%1.1f%%\",\n    colors=colors_pie, startangle=90,\n    textprops={\"fontsize\": 11},\n)\naxes[0].set_title(\"ANOVA: Variance Decomposition\", fontsize=13)\n\n# Bar chart\naxes[1].barh(sources, contributions, color=colors_pie, edgecolor=\"black\", linewidth=0.5)\naxes[1].set_xlabel(\"% Contribution to Model SS\", fontsize=12)\naxes[1].set_title(\"ANOVA: Term Contributions\", fontsize=13)\nfor i, v in enumerate(contributions):\n    axes[1].text(v + 0.5, i, f\"{v:.1f}%\", va=\"center\", fontsize=11)\naxes[1].set_xlim(0, max(contributions) * 1.15)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "step7-header",
   "metadata": {},
   "source": [
    "## Step 7: Final Report\n",
    "\n",
    "Summary of the Langmuir isotherm parameter estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"╔\" + \"═\" * 60 + \"╗\")\n",
    "print(\"║\" + \" LANGMUIR ISOTHERM — FINAL REPORT\".center(60) + \"║\")\n",
    "print(\"╠\" + \"═\" * 60 + \"╣\")\n",
    "print(f\"║  {'Model:':12s} {model.expression_:>45s}  ║\")\n",
    "print(f\"║  {'Data points:':12s} {len(y_all):>45d}  ║\")\n",
    "print(f\"║  {'Rounds:':12s} {N_ROUNDS + 1:>45d}  ║\")\n",
    "print(f\"║  {'R²:':12s} {model.metrics_['r2']:>45.6f}  ║\")\n",
    "print(f\"║  {'MSE:':12s} {model.metrics_['mse']:>45.6g}  ║\")\n",
    "print(f\"║  {'AICc:':12s} {model.metrics_['aicc']:>45.2f}  ║\")\n",
    "print(f\"║  {'sigma:':12s} {model.sigma_:>45.4f}  ║\")\n",
    "print(\"╠\" + \"═\" * 60 + \"╣\")\n",
    "\n",
    "# Parameter summary\n",
    "print(\"║\" + \" Parameter Estimates\".center(60) + \"║\")\n",
    "print(\"║\" + \"-\" * 60 + \"║\")\n",
    "for name, coef in zip(model.selected_features_, model.coefficients_, strict=False):\n",
    "    ci = intervals.get(name, (float(\"nan\"), float(\"nan\")))\n",
    "    print(f\"║  {name:25s} = {float(coef):8.4f}  CI: [{ci[0]:7.4f}, {ci[1]:7.4f}]  ║\")\n",
    "\n",
    "print(\"╠\" + \"═\" * 60 + \"╣\")\n",
    "print(\"║\" + \" Comparison to True Values\".center(60) + \"║\")\n",
    "print(\"║\" + \"-\" * 60 + \"║\")\n",
    "print(f\"║  {'q_max:':10s}  estimated = {q_max_est:7.4f},  true = {Q_MAX_TRUE:7.4f}\" + \" \" * 10 + \"║\")\n",
    "print(f\"║  {'K:':10s}  (embedded in basis function)\" + \" \" * 18 + \"║\")\n",
    "print(\"╠\" + \"═\" * 60 + \"╣\")\n",
    "\n",
    "# LaTeX\n",
    "print(\"║\" + \" LaTeX Equation\".center(60) + \"║\")\n",
    "print(\"║\" + \"-\" * 60 + \"║\")\n",
    "latex_eq = model.to_latex()\n",
    "print(f\"║  ${latex_eq}$\")\n",
    "print(\"╚\" + \"═\" * 60 + \"╝\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison: all UQ methods\n",
    "P_final = np.linspace(0.01, 10.0, 100).reshape(-1, 1)\n",
    "P_jax = jnp.array(P_final)\n",
    "\n",
    "# OLS prediction interval\n",
    "y_ols, ols_lo, ols_hi = model.predict_interval(P_jax, alpha=0.05)\n",
    "\n",
    "# Bootstrap prediction interval\n",
    "boot_final = bootstrap_predict(model, P_jax, n_bootstrap=500, alpha=0.05, seed=42)\n",
    "\n",
    "# Conformal prediction interval\n",
    "y_conf_f, conf_lo_f, conf_hi_f = model.predict_conformal(P_jax, alpha=0.05, method=\"jackknife+\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, (lo, hi, title) in zip(\n",
    "    axes,\n",
    "    [\n",
    "        (ols_lo, ols_hi, \"OLS Prediction Interval\"),\n",
    "        (boot_final.lower, boot_final.upper, \"Bootstrap Interval\"),\n",
    "        (conf_lo_f, conf_hi_f, \"Conformal (jackknife+)\"),\n",
    "    ],\n",
    "    strict=False,\n",
    "):\n",
    "    ax.fill_between(P_final.flatten(), np.asarray(lo), np.asarray(hi),\n",
    "                    alpha=0.2, color=\"blue\", label=\"95% interval\")\n",
    "    ax.plot(P_final, np.asarray(y_ols), \"b-\", linewidth=1.5, label=\"Predicted\")\n",
    "    ax.plot(P_dense, q_dense, \"k--\", alpha=0.4, label=\"True\")\n",
    "    ax.scatter(X_all[:, 0], y_all, color=\"red\", s=20, alpha=0.7, zorder=5, label=\"Data\")\n",
    "    ax.set_xlabel(\"Pressure (bar)\")\n",
    "    ax.set_ylabel(\"q (mol/g)\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend(fontsize=8, loc=\"lower right\")\n",
    "\n",
    "    width = np.mean(np.asarray(hi) - np.asarray(lo))\n",
    "    ax.text(0.05, 0.95, f\"Avg width: {width:.3f}\",\n",
    "            transform=ax.transAxes, fontsize=10, va=\"top\",\n",
    "            bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.5))\n",
    "\n",
    "plt.suptitle(\"Uncertainty Quantification Comparison\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save artifacts\n",
    "study.add_observations(X_all[8:], y_all[8:], notes=\"Active learning rounds 1-4\")\n",
    "study.save(\"langmuir_study.jaxsr\")\n",
    "model.save(\"langmuir_model.json\")\n",
    "\n",
    "# Export callable for deployment\n",
    "predict_fn = model.to_callable()\n",
    "P_check = np.array([[1.0], [5.0]])\n",
    "q_check = predict_fn(P_check)\n",
    "print(\"Pure NumPy callable (no JAX dependency):\")\n",
    "print(f\"  q(P=1.0) = {q_check[0]:.4f}  (true: {Q_MAX_TRUE * K_TRUE * 1.0 / (1 + K_TRUE * 1.0):.4f})\")\n",
    "print(f\"  q(P=5.0) = {q_check[1]:.4f}  (true: {Q_MAX_TRUE * K_TRUE * 5.0 / (1 + K_TRUE * 5.0):.4f})\")\n",
    "\n",
    "print(f\"\\nStudy saved to: langmuir_study.jaxsr\")\n",
    "print(f\"Model saved to: langmuir_model.json\")\n",
    "\n",
    "# Clean up\n",
    "import os\n",
    "for f in [\"langmuir_study.jaxsr\", \"langmuir_model.json\"]:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **DOE design** — Started with 8 Latin Hypercube points covering the pressure range\n",
    "2. **Parametric basis** — Encoded the known Langmuir form using `add_parametric`, letting JAXSR optimize $K$ by profile likelihood and estimate $q_{\\max}$ by OLS\n",
    "3. **Active learning** — Used uncertainty-based adaptive sampling to add 3 targeted experiments per round, focusing measurements where the model was most uncertain\n",
    "4. **Multiple UQ methods** — Compared OLS intervals, bootstrap, and conformal prediction\n",
    "5. **ANOVA** — Confirmed the Langmuir term explains virtually all variance, validating model adequacy\n",
    "6. **Export** — Extracted LaTeX equation, saved model, and created a pure NumPy callable\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Active learning reduces experimental cost** — 20 targeted measurements can be as informative as 50+ random ones\n",
    "- **Parametric basis functions** let you encode known model forms while still using JAXSR's selection and UQ machinery\n",
    "- **ANOVA validates the model** — if the primary term doesn't dominate, the model form may be wrong\n",
    "- **Multiple UQ methods give different views** — OLS is fast, bootstrap captures nonlinear parameter uncertainty, conformal gives distribution-free guarantees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}