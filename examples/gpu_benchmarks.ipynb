{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAXSR Performance: CPU vs GPU Benchmarking\n",
    "\n",
    "JAXSR uses JAX for its core linear algebra operations (`lstsq`, SVD, `pinv`, `matmul`),\n",
    "which are transparently accelerated on GPU when available. JAX dispatches these operations\n",
    "to device-specific BLAS kernels — cuBLAS/cuSOLVER on GPU, MKL/OpenBLAS on CPU.\n",
    "\n",
    "**Key points:**\n",
    "- GPU advantage grows with problem size. Small problems may be faster on CPU due to kernel launch overhead.\n",
    "- Python-level loops (greedy selection iterations, basis function evaluation) run on CPU regardless;\n",
    "  GPU accelerates the individual JAX operations *within* those loops.\n",
    "- This notebook benchmarks 6 JAXSR features across varying problem sizes to show when GPU acceleration matters.\n",
    "\n",
    "If no GPU is available, the notebook still runs and reports CPU-only timings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\nfrom jaxsr import (\n    BasisLibrary,\n    SymbolicRegressor,\n    cross_validate,\n    bootstrap_model_selection,\n    discover_dynamics,\n)\n\n# --- Device detection ---\ncpu_device = jax.devices(\"cpu\")[0]\ntry:\n    gpu_device = jax.devices(\"gpu\")[0]\n    HAS_GPU = True\nexcept RuntimeError:\n    HAS_GPU = False\n    gpu_device = None\n\n\n# --- Benchmark utility ---\ndef benchmark(fn, device, warmup=1, repeats=5):\n    \"\"\"Time a function on the given JAX device.\n\n    Runs `warmup` calls to trigger JIT compilation, then times `repeats` runs\n    and returns the median wall-clock time in seconds.\n    \"\"\"\n    with jax.default_device(device):\n        # Warmup (JIT compilation)\n        for _ in range(warmup):\n            fn()\n            jnp.zeros(1).block_until_ready()\n\n        # Timed runs\n        times = []\n        for _ in range(repeats):\n            start = time.perf_counter()\n            fn()\n            jnp.zeros(1).block_until_ready()\n            elapsed = time.perf_counter() - start\n            times.append(elapsed)\n\n    return np.median(times)\n\n\n# --- Results collector ---\nresults = []"
  },
  {
   "cell_type": "code",
   "source": "import os\nimport platform\nimport subprocess\n\nprint(\"System Information\")\nprint(\"=\" * 60)\n\n# OS info\nprint(f\"OS:           {platform.system()} {platform.release()}\")\nprint(f\"Platform:     {platform.platform()}\")\nprint(f\"Python:       {platform.python_version()}\")\n\n# CPU info\nprint(f\"\\nCPU:          {platform.processor() or 'unknown'}\")\ncpu_count_physical = os.cpu_count()\nprint(f\"CPU cores:    {cpu_count_physical}\")\ntry:\n    with open(\"/proc/cpuinfo\") as f:\n        for line in f:\n            if line.startswith(\"model name\"):\n                print(f\"CPU model:    {line.split(':')[1].strip()}\")\n                break\nexcept FileNotFoundError:\n    pass\n\n# Memory info\ntry:\n    with open(\"/proc/meminfo\") as f:\n        for line in f:\n            if line.startswith(\"MemTotal\"):\n                mem_kb = int(line.split()[1])\n                print(f\"\\nMemory:       {mem_kb / 1024 / 1024:.1f} GB\")\n                break\nexcept FileNotFoundError:\n    pass\n\n# GPU info\nprint(f\"\\nJAX version:  {jax.__version__}\")\nprint(f\"JAX backend:  {jax.default_backend()}\")\nprint(f\"CPU device:   {cpu_device}\")\nif HAS_GPU:\n    print(f\"GPU device:   {gpu_device}\")\n    try:\n        nvidia_out = subprocess.check_output(\n            [\"nvidia-smi\", \"--query-gpu=name,memory.total,driver_version\", \"--format=csv,noheader\"],\n            text=True,\n        ).strip()\n        for line in nvidia_out.split(\"\\n\"):\n            parts = [p.strip() for p in line.split(\",\")]\n            if len(parts) >= 3:\n                print(f\"GPU model:    {parts[0]}\")\n                print(f\"GPU memory:   {parts[1]}\")\n                print(f\"NVIDIA driver:{parts[2]}\")\n    except (FileNotFoundError, subprocess.CalledProcessError):\n        print(\"GPU details:  nvidia-smi not available\")\nelse:\n    print(\"GPU device:   Not available\")\n\nprint(f\"\\nNumPy:        {np.__version__}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 1: Basis Library Evaluation\n",
    "\n",
    "**What:** `BasisLibrary.evaluate(X)` constructs the design matrix $\\Phi$ by evaluating\n",
    "each basis function on the input data.\n",
    "\n",
    "**Why it matters:** This is the first step in every JAXSR workflow. The `evaluate()` call\n",
    "loops over each basis function in Python and calls `jnp.column_stack()`. Each elementwise\n",
    "op (e.g., `jnp.log`, `jnp.exp`, `x**3`) runs on the device, so GPU wins when `n_samples`\n",
    "is large enough to amortize kernel launch overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Benchmark 1: Basis Library Evaluation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Build a large library with 5 features\n",
    "library = (\n",
    "    BasisLibrary(n_features=5)\n",
    "    .add_constant()\n",
    "    .add_linear()\n",
    "    .add_polynomials(max_degree=4)\n",
    "    .add_interactions(max_order=3)\n",
    "    .add_transcendental([\"log\", \"exp\", \"sqrt\", \"inv\"])\n",
    ")\n",
    "print(f\"Library size: {len(library.names)} basis functions\")\n",
    "\n",
    "sizes = [1_000, 10_000, 100_000]\n",
    "\n",
    "for n in sizes:\n",
    "    rng = np.random.default_rng(42)\n",
    "    # Positive values needed for log/sqrt/inv\n",
    "    X_np = rng.uniform(0.1, 5.0, size=(n, 5))\n",
    "\n",
    "    cpu_time = benchmark(lambda: library.evaluate(X_np), cpu_device, warmup=1, repeats=5)\n",
    "    gpu_time = benchmark(lambda: library.evaluate(X_np), gpu_device, warmup=1, repeats=5) if HAS_GPU else None\n",
    "\n",
    "    speedup = cpu_time / gpu_time if gpu_time else None\n",
    "    gpu_str = f\"{gpu_time:.4f}s\" if gpu_time else \"N/A\"\n",
    "    sp_str = f\"{speedup:.2f}x\" if speedup else \"N/A\"\n",
    "    print(f\"  n={n:>7,}: CPU={cpu_time:.4f}s  GPU={gpu_str}  Speedup={sp_str}\")\n",
    "\n",
    "    results.append({\n",
    "        \"benchmark\": \"Basis Evaluation\",\n",
    "        \"size\": n,\n",
    "        \"cpu\": cpu_time,\n",
    "        \"gpu\": gpu_time,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 2: Model Fitting — Greedy Forward Selection\n",
    "\n",
    "**What:** `SymbolicRegressor.fit()` with `strategy=\"greedy_forward\"` iteratively adds\n",
    "basis functions that most improve the fit. Each iteration evaluates all remaining candidates\n",
    "via `lstsq` calls.\n",
    "\n",
    "**Why it matters:** This is the primary fitting workflow. With ~50 basis functions and\n",
    "`max_terms=8`, greedy forward evaluates hundreds of `lstsq` calls. At large `n_samples`,\n",
    "the GPU BLAS kernel for `lstsq` should clearly outperform CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Benchmark 2: Greedy Forward Selection\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "sizes = [500, 5_000, 50_000]\n",
    "\n",
    "for n in sizes:\n",
    "    rng = np.random.default_rng(42)\n",
    "    X_np = rng.uniform(0.1, 5.0, size=(n, 4))\n",
    "    x0, x1, x2, x3 = X_np[:, 0], X_np[:, 1], X_np[:, 2], X_np[:, 3]\n",
    "    y_np = 2.0 * x0 + 1.5 * x1**2 - 0.8 * x2 * x3 + 0.3 + rng.normal(0, 0.1, n)\n",
    "\n",
    "    lib = (\n",
    "        BasisLibrary(n_features=4)\n",
    "        .add_constant()\n",
    "        .add_linear()\n",
    "        .add_polynomials(max_degree=3)\n",
    "        .add_interactions(max_order=2)\n",
    "        .add_transcendental([\"log\", \"exp\", \"sqrt\", \"inv\"])\n",
    "    )\n",
    "\n",
    "    def run_greedy():\n",
    "        model = SymbolicRegressor(\n",
    "            basis_library=lib, max_terms=8, strategy=\"greedy_forward\",\n",
    "        )\n",
    "        model.fit(X_np, y_np)\n",
    "\n",
    "    cpu_time = benchmark(run_greedy, cpu_device, warmup=1, repeats=3)\n",
    "    gpu_time = benchmark(run_greedy, gpu_device, warmup=1, repeats=3) if HAS_GPU else None\n",
    "\n",
    "    speedup = cpu_time / gpu_time if gpu_time else None\n",
    "    gpu_str = f\"{gpu_time:.4f}s\" if gpu_time else \"N/A\"\n",
    "    sp_str = f\"{speedup:.2f}x\" if speedup else \"N/A\"\n",
    "    print(f\"  n={n:>7,}: CPU={cpu_time:.4f}s  GPU={gpu_str}  Speedup={sp_str}\")\n",
    "\n",
    "    results.append({\n",
    "        \"benchmark\": \"Greedy Forward\",\n",
    "        \"size\": n,\n",
    "        \"cpu\": cpu_time,\n",
    "        \"gpu\": gpu_time,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 3: Exhaustive Model Search\n",
    "\n",
    "**What:** `SymbolicRegressor.fit()` with `strategy=\"exhaustive\"` evaluates all subsets\n",
    "$\\binom{B}{k}$ for $k = 1, \\ldots, \\text{max\\_terms}$.\n",
    "\n",
    "**Why it matters:** This is the most computation-dense benchmark. With 10 basis functions\n",
    "and `max_terms=5`, there are $\\binom{10}{1} + \\cdots + \\binom{10}{5} = 637$ `lstsq` calls.\n",
    "Pure computation with minimal Python overhead between calls — best case for GPU advantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Benchmark 3: Exhaustive Model Search\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "sizes = [1_000, 10_000, 100_000]\n",
    "\n",
    "for n in sizes:\n",
    "    rng = np.random.default_rng(42)\n",
    "    X_np = rng.uniform(0.1, 5.0, size=(n, 2))\n",
    "    x0, x1 = X_np[:, 0], X_np[:, 1]\n",
    "    y_np = 3.0 * x0**2 - 1.5 * x0 * x1 + 0.5 + rng.normal(0, 0.1, n)\n",
    "\n",
    "    lib = (\n",
    "        BasisLibrary(n_features=2)\n",
    "        .add_constant()\n",
    "        .add_linear()\n",
    "        .add_polynomials(max_degree=3)\n",
    "        .add_interactions(max_order=2)\n",
    "    )\n",
    "    print(f\"  Library size: {len(lib.names)} basis functions\")\n",
    "\n",
    "    def run_exhaustive():\n",
    "        model = SymbolicRegressor(\n",
    "            basis_library=lib, max_terms=5, strategy=\"exhaustive\",\n",
    "        )\n",
    "        model.fit(X_np, y_np)\n",
    "\n",
    "    cpu_time = benchmark(run_exhaustive, cpu_device, warmup=1, repeats=3)\n",
    "    gpu_time = benchmark(run_exhaustive, gpu_device, warmup=1, repeats=3) if HAS_GPU else None\n",
    "\n",
    "    speedup = cpu_time / gpu_time if gpu_time else None\n",
    "    gpu_str = f\"{gpu_time:.4f}s\" if gpu_time else \"N/A\"\n",
    "    sp_str = f\"{speedup:.2f}x\" if speedup else \"N/A\"\n",
    "    print(f\"  n={n:>7,}: CPU={cpu_time:.4f}s  GPU={gpu_str}  Speedup={sp_str}\")\n",
    "\n",
    "    results.append({\n",
    "        \"benchmark\": \"Exhaustive Search\",\n",
    "        \"size\": n,\n",
    "        \"cpu\": cpu_time,\n",
    "        \"gpu\": gpu_time,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 4: Cross-Validation\n",
    "\n",
    "**What:** `cross_validate(model, X, y, cv=10)` performs 10-fold cross-validation.\n",
    "Each fold clones the model and does a full `fit()` on ~90% of the data.\n",
    "\n",
    "**Why it matters:** 10 independent model fits multiply the GPU advantage from\n",
    "Benchmark 2 by approximately 10x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Benchmark 4: Cross-Validation (10-fold)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "sizes = [1_000, 10_000, 50_000]\n",
    "\n",
    "for n in sizes:\n",
    "    rng = np.random.default_rng(42)\n",
    "    X_np = rng.uniform(0.1, 5.0, size=(n, 4))\n",
    "    x0, x1, x2, x3 = X_np[:, 0], X_np[:, 1], X_np[:, 2], X_np[:, 3]\n",
    "    y_np = 2.0 * x0 + 1.5 * x1**2 - 0.8 * x2 * x3 + 0.3 + rng.normal(0, 0.1, n)\n",
    "\n",
    "    lib = (\n",
    "        BasisLibrary(n_features=4)\n",
    "        .add_constant()\n",
    "        .add_linear()\n",
    "        .add_polynomials(max_degree=3)\n",
    "        .add_interactions(max_order=2)\n",
    "    )\n",
    "\n",
    "    model = SymbolicRegressor(\n",
    "        basis_library=lib, max_terms=8, strategy=\"greedy_forward\",\n",
    "    )\n",
    "\n",
    "    def run_cv():\n",
    "        cross_validate(model, X_np, y_np, cv=10, random_state=42)\n",
    "\n",
    "    cpu_time = benchmark(run_cv, cpu_device, warmup=0, repeats=3)\n",
    "    gpu_time = benchmark(run_cv, gpu_device, warmup=0, repeats=3) if HAS_GPU else None\n",
    "\n",
    "    speedup = cpu_time / gpu_time if gpu_time else None\n",
    "    gpu_str = f\"{gpu_time:.4f}s\" if gpu_time else \"N/A\"\n",
    "    sp_str = f\"{speedup:.2f}x\" if speedup else \"N/A\"\n",
    "    print(f\"  n={n:>7,}: CPU={cpu_time:.4f}s  GPU={gpu_str}  Speedup={sp_str}\")\n",
    "\n",
    "    results.append({\n",
    "        \"benchmark\": \"Cross-Validation\",\n",
    "        \"size\": n,\n",
    "        \"cpu\": cpu_time,\n",
    "        \"gpu\": gpu_time,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 5: Bootstrap Model Stability\n",
    "\n",
    "**What:** `bootstrap_model_selection(model, X, y, n_bootstrap=N)` resamples the data\n",
    "N times and refits the model each time to assess selection stability.\n",
    "\n",
    "**Why it matters:** Each bootstrap iteration clones the model and calls `fit()` on\n",
    "a resampled dataset. Similar to cross-validation but with more iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Benchmark 5: Bootstrap Model Stability\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "n = 2_000\n",
    "rng = np.random.default_rng(42)\n",
    "X_np = rng.uniform(0.1, 5.0, size=(n, 4))\n",
    "x0, x1, x2, x3 = X_np[:, 0], X_np[:, 1], X_np[:, 2], X_np[:, 3]\n",
    "y_np = 2.0 * x0 + 1.5 * x1**2 - 0.8 * x2 * x3 + 0.3 + rng.normal(0, 0.1, n)\n",
    "\n",
    "lib = (\n",
    "    BasisLibrary(n_features=4)\n",
    "    .add_constant()\n",
    "    .add_linear()\n",
    "    .add_polynomials(max_degree=3)\n",
    "    .add_interactions(max_order=2)\n",
    ")\n",
    "\n",
    "model = SymbolicRegressor(\n",
    "    basis_library=lib, max_terms=8, strategy=\"greedy_forward\",\n",
    ")\n",
    "# Fit once so bootstrap_model_selection can clone from a fitted model\n",
    "with jax.default_device(cpu_device):\n",
    "    model.fit(X_np, y_np)\n",
    "\n",
    "bootstrap_sizes = [20, 50]\n",
    "\n",
    "for n_boot in bootstrap_sizes:\n",
    "    def run_bootstrap():\n",
    "        bootstrap_model_selection(model, X_np, y_np, n_bootstrap=n_boot, seed=42)\n",
    "\n",
    "    cpu_time = benchmark(run_bootstrap, cpu_device, warmup=0, repeats=3)\n",
    "    gpu_time = benchmark(run_bootstrap, gpu_device, warmup=0, repeats=3) if HAS_GPU else None\n",
    "\n",
    "    speedup = cpu_time / gpu_time if gpu_time else None\n",
    "    gpu_str = f\"{gpu_time:.4f}s\" if gpu_time else \"N/A\"\n",
    "    sp_str = f\"{speedup:.2f}x\" if speedup else \"N/A\"\n",
    "    print(f\"  n_bootstrap={n_boot:>3}: CPU={cpu_time:.4f}s  GPU={gpu_str}  Speedup={sp_str}\")\n",
    "\n",
    "    results.append({\n",
    "        \"benchmark\": \"Bootstrap Stability\",\n",
    "        \"size\": n_boot,\n",
    "        \"cpu\": cpu_time,\n",
    "        \"gpu\": gpu_time,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 6: ODE/Dynamics Discovery\n",
    "\n",
    "**What:** `discover_dynamics(X, t, ...)` estimates derivatives from time-series data,\n",
    "then fits one `SymbolicRegressor` per state variable.\n",
    "\n",
    "**Setup:** Lotka-Volterra predator-prey system:\n",
    "$$\\frac{dx}{dt} = \\alpha x - \\beta xy, \\quad \\frac{dy}{dt} = \\delta xy - \\gamma y$$\n",
    "\n",
    "**Why it matters:** Mixed workload — derivative estimation uses NumPy/SciPy (always CPU),\n",
    "but the symbolic regression fits use JAX. Shows a realistic scientific workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Benchmark 6: ODE/Dynamics Discovery\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Lotka-Volterra parameters\n",
    "alpha, beta, delta, gamma = 1.0, 0.1, 0.075, 1.5\n",
    "\n",
    "\n",
    "def lotka_volterra(t, z):\n",
    "    x, y = z\n",
    "    return [alpha * x - beta * x * y, delta * x * y - gamma * y]\n",
    "\n",
    "\n",
    "sizes = [500, 5_000, 50_000]\n",
    "\n",
    "for n_pts in sizes:\n",
    "    t_span = (0.0, 15.0)\n",
    "    t_eval = np.linspace(*t_span, n_pts)\n",
    "    sol = solve_ivp(lotka_volterra, t_span, [10.0, 5.0], t_eval=t_eval, method=\"RK45\")\n",
    "    X_dyn = sol.y.T  # shape (n_pts, 2)\n",
    "    t_arr = sol.t\n",
    "\n",
    "    def run_dynamics():\n",
    "        discover_dynamics(\n",
    "            X_dyn, t_arr,\n",
    "            state_names=[\"prey\", \"predator\"],\n",
    "            max_terms=5,\n",
    "            strategy=\"greedy_forward\",\n",
    "        )\n",
    "\n",
    "    cpu_time = benchmark(run_dynamics, cpu_device, warmup=0, repeats=3)\n",
    "    gpu_time = benchmark(run_dynamics, gpu_device, warmup=0, repeats=3) if HAS_GPU else None\n",
    "\n",
    "    speedup = cpu_time / gpu_time if gpu_time else None\n",
    "    gpu_str = f\"{gpu_time:.4f}s\" if gpu_time else \"N/A\"\n",
    "    sp_str = f\"{speedup:.2f}x\" if speedup else \"N/A\"\n",
    "    print(f\"  n_pts={n_pts:>7,}: CPU={cpu_time:.4f}s  GPU={gpu_str}  Speedup={sp_str}\")\n",
    "\n",
    "    results.append({\n",
    "        \"benchmark\": \"ODE Discovery\",\n",
    "        \"size\": n_pts,\n",
    "        \"cpu\": cpu_time,\n",
    "        \"gpu\": gpu_time,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Summary Table ---\n",
    "print(\"\\nPerformance Summary\")\n",
    "print(\"=\" * 75)\n",
    "header = f\"{'Benchmark':<22} {'Size':>10} {'CPU (s)':>10} {'GPU (s)':>10} {'Speedup':>10}\"\n",
    "print(header)\n",
    "print(\"-\" * 75)\n",
    "for r in results:\n",
    "    gpu_str = f\"{r['gpu']:.4f}\" if r[\"gpu\"] is not None else \"N/A\"\n",
    "    speedup = r[\"cpu\"] / r[\"gpu\"] if r[\"gpu\"] else None\n",
    "    sp_str = f\"{speedup:.2f}x\" if speedup else \"N/A\"\n",
    "    print(f\"{r['benchmark']:<22} {r['size']:>10,} {r['cpu']:>10.4f} {gpu_str:>10} {sp_str:>10}\")\n",
    "\n",
    "# --- Visualization ---\n",
    "# Use the largest problem size for each benchmark\n",
    "benchmarks_seen = []\n",
    "largest = {}\n",
    "for r in results:\n",
    "    name = r[\"benchmark\"]\n",
    "    if name not in largest or r[\"size\"] > largest[name][\"size\"]:\n",
    "        largest[name] = r\n",
    "    if name not in benchmarks_seen:\n",
    "        benchmarks_seen.append(name)\n",
    "\n",
    "bench_names = benchmarks_seen\n",
    "cpu_times = [largest[b][\"cpu\"] for b in bench_names]\n",
    "gpu_times = [largest[b][\"gpu\"] for b in bench_names]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: CPU vs GPU bar chart\n",
    "ax = axes[0]\n",
    "x_pos = np.arange(len(bench_names))\n",
    "bar_width = 0.35\n",
    "ax.bar(x_pos - bar_width / 2, cpu_times, bar_width, label=\"CPU\", color=\"steelblue\")\n",
    "if HAS_GPU:\n",
    "    gpu_vals = [g if g is not None else 0 for g in gpu_times]\n",
    "    ax.bar(x_pos + bar_width / 2, gpu_vals, bar_width, label=\"GPU\", color=\"coral\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_ylabel(\"Time (s, log scale)\")\n",
    "ax.set_title(\"CPU vs GPU — Largest Problem Size\")\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(bench_names, rotation=30, ha=\"right\", fontsize=8)\n",
    "ax.legend()\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Plot 2: Speedup bar chart\n",
    "ax = axes[1]\n",
    "if HAS_GPU:\n",
    "    speedups = [\n",
    "        largest[b][\"cpu\"] / largest[b][\"gpu\"]\n",
    "        if largest[b][\"gpu\"] is not None\n",
    "        else 0\n",
    "        for b in bench_names\n",
    "    ]\n",
    "    colors = [\"seagreen\" if s > 1 else \"indianred\" for s in speedups]\n",
    "    ax.barh(bench_names, speedups, color=colors)\n",
    "    ax.axvline(x=1.0, color=\"black\", linestyle=\"--\", linewidth=1, label=\"Break-even\")\n",
    "    ax.set_xlabel(\"Speedup (CPU time / GPU time)\")\n",
    "    ax.set_title(\"GPU Speedup — Largest Problem Size\")\n",
    "    ax.legend()\n",
    "    ax.grid(axis=\"x\", alpha=0.3)\n",
    "else:\n",
    "    ax.text(\n",
    "        0.5, 0.5, \"No GPU available\\nSpeedup chart requires GPU\",\n",
    "        ha=\"center\", va=\"center\", transform=ax.transAxes, fontsize=12,\n",
    "    )\n",
    "    ax.set_title(\"GPU Speedup — N/A\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways\n\n1. **GPU overhead dominates for small problems.** JAXSR's core workflow involves many\n   small `lstsq` calls inside Python loops (greedy selection, exhaustive search). Each\n   GPU kernel launch has fixed overhead (~0.1–1 ms), and when the matrices are small,\n   this overhead exceeds the computation time. CPU avoids this overhead entirely.\n\n2. **GPU only helps at very large `n_samples`.** Basis evaluation at 100K samples showed\n   1.7x speedup, and exhaustive search at 100K showed 1.4x. The crossover point where\n   GPU matches CPU is roughly 50K–100K samples for most workflows.\n\n3. **Python-level loops are the real bottleneck (Amdahl's law).** Greedy forward selection\n   iterates in Python over candidate basis functions. Even with instant linear algebra,\n   the loop overhead caps speedup. This is why bootstrap (50 full fits) showed 0.26x —\n   the overhead multiplies with iteration count.\n\n4. **Basis evaluation benefits most.** This is the most \"GPU-friendly\" operation: each\n   basis function is an elementwise op on a large array, with minimal Python loop overhead\n   relative to computation.\n\n5. **The honest conclusion: for typical JAXSR workloads, CPU is faster.** Unless you\n   are fitting models with >50K samples, stick with CPU. Set `JAX_PLATFORMS=cpu` to\n   avoid GPU kernel launch overhead:\n   ```python\n   import os\n   os.environ[\"JAX_PLATFORMS\"] = \"cpu\"\n   ```\n\n6. **Where GPU *would* help.** If JAXSR's inner loops were replaced with batched/vmapped\n   JAX operations (e.g., vmapping lstsq over all candidate subsets at once), the GPU\n   advantage would be dramatic. This is a potential future optimization.\n\n7. **Vectorized bootstrap functions are already efficient.** `bootstrap_coefficients()` and\n   `bootstrap_predict()` compute the pseudo-inverse once and apply it to all bootstrap\n   samples in a single matmul — so the per-iteration cost is negligible regardless of device."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}