{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc9aef00",
   "metadata": {},
   "source": [
    "# Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d3dff8",
   "metadata": {},
   "source": [
    "Active Learning & Acquisition Functions Example for JAXSR.\n",
    "\n",
    "Demonstrates how to use acquisition functions to intelligently select\n",
    "the next experiments to run, balancing exploration and exploitation.\n",
    "\n",
    "This example covers five scenarios:\n",
    "\n",
    "1. Pure exploration: reduce uncertainty everywhere\n",
    "2. Bayesian optimisation: find the minimum of an unknown function\n",
    "3. Model discrimination: resolve which model structure is correct\n",
    "4. Batch selection strategies: greedy vs penalized vs kriging believer\n",
    "5. Full active learning loop: iteratively improve a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7332f0f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T22:37:16.283326Z",
     "iopub.status.busy": "2026-02-10T22:37:16.283266Z",
     "iopub.status.idle": "2026-02-10T22:37:16.735536Z",
     "shell.execute_reply": "2026-02-10T22:37:16.735115Z"
    }
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jaxsr import BasisLibrary, SymbolicRegressor\n",
    "from jaxsr.acquisition import (\n",
    "    LCB,\n",
    "    ActiveLearner,\n",
    "    AOptimal,\n",
    "    BMAUncertainty,\n",
    "    ConfidenceBandWidth,\n",
    "    DOptimal,\n",
    "    EnsembleDisagreement,\n",
    "    ExpectedImprovement,\n",
    "    ModelDiscrimination,\n",
    "    ModelMin,\n",
    "    PredictionVariance,\n",
    "    ProbabilityOfImprovement,\n",
    "    ThompsonSampling,\n",
    "    suggest_points,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62f0529",
   "metadata": {},
   "source": [
    "## Setup: Create a fitted model\n",
    "\n",
    "We'll use this model across all examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41c382fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T22:37:16.737091Z",
     "iopub.status.busy": "2026-02-10T22:37:16.736971Z",
     "iopub.status.idle": "2026-02-10T22:37:18.149307Z",
     "shell.execute_reply": "2026-02-10T22:37:18.148820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SymbolicRegressor(max_terms=4, strategy='greedy_forward', fitted)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.uniform(0, 5, (40, 1))\n",
    "y = X[:, 0] ** 2 - 3.0 * X[:, 0] + 2.0 + np.random.randn(40) * 0.3\n",
    "\n",
    "library = (\n",
    "    BasisLibrary(n_features=1, feature_names=[\"x\"])\n",
    "    .add_constant()\n",
    "    .add_linear()\n",
    "    .add_polynomials(max_degree=3)\n",
    ")\n",
    "model = SymbolicRegressor(basis_library=library, max_terms=4, strategy=\"greedy_forward\")\n",
    "model.fit(jnp.array(X), jnp.array(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579953cc",
   "metadata": {},
   "source": [
    "## Goal: improve model accuracy uniformly by sampling where prediction uncertainty is highest\n",
    "\n",
    "    Available acquisition functions for this goal:\n",
    "\n",
    "    - PredictionVariance: the default.  Uses the OLS posterior to compute\n",
    "      sigma^2(x).  Fast, exact for linear-in-parameter models.\n",
    "\n",
    "    - ConfidenceBandWidth: similar, but reports the actual width of the\n",
    "      confidence band at a specified significance level.\n",
    "\n",
    "    - EnsembleDisagreement: uses the Pareto front of models with different\n",
    "      complexities.  Good when you're unsure whether a simpler or more\n",
    "      complex model is appropriate.\n",
    "\n",
    "    - BMAUncertainty: Bayesian Model Averaging.  The most comprehensive\n",
    "      measure -- combines noise uncertainty and model-selection uncertainty.\n",
    "\n",
    "    - AOptimal: targets reduction in *parameter* uncertainty (trace of\n",
    "      covariance matrix).  Use when you care about accurate coefficients.\n",
    "\n",
    "    - DOptimal: maximises information gain (det of information matrix).\n",
    "      Use when you want maximal information per experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36ad849b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T22:37:18.150448Z",
     "iopub.status.busy": "2026-02-10T22:37:18.150375Z",
     "iopub.status.idle": "2026-02-10T22:37:18.924351Z",
     "shell.execute_reply": "2026-02-10T22:37:18.923868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model: y = 0.0132*x^3 + 0.8891*x^2 + 1.827 - 2.736*x\n",
      "Current R^2:   0.9941\n",
      "Training size: 40\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PredictionVariance             -> x = [5.00, 0.01, 0.00]\n",
      "  ConfidenceBandWidth(95%)       -> x = [5.00, 0.01, 0.00]\n",
      "  EnsembleDisagreement           -> x = [0.01, 0.01, 0.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BMAUncertainty                 -> x = [4.95, 1.69, 1.59]\n",
      "  AOptimal                       -> x = [0.01, 0.01, 0.00]\n",
      "  DOptimal                       -> x = [5.00, 0.01, 0.00]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bounds = [(0.0, 5.0)]\n",
    "\n",
    "print(f\"Current model: {model.expression_}\")\n",
    "print(f\"Current R^2:   {model.score(model._X_train, model._y_train):.4f}\")\n",
    "print(f\"Training size: {len(model._y_train)}\")\n",
    "print()\n",
    "\n",
    "# --- Try different exploration strategies ---\n",
    "strategies = [\n",
    "    (\"PredictionVariance\", PredictionVariance()),\n",
    "    (\"ConfidenceBandWidth(95%)\", ConfidenceBandWidth(alpha=0.05)),\n",
    "    (\"EnsembleDisagreement\", EnsembleDisagreement()),\n",
    "    (\"BMAUncertainty\", BMAUncertainty(criterion=\"bic\")),\n",
    "    (\"AOptimal\", AOptimal()),\n",
    "    (\"DOptimal\", DOptimal()),\n",
    "]\n",
    "\n",
    "for name, acq in strategies:\n",
    "    result = suggest_points(model, bounds, acq, n_points=3, random_state=42)\n",
    "    pts = np.array(result.points).ravel()\n",
    "    print(f\"  {name:30s} -> x = [{', '.join(f'{p:.2f}' for p in pts)}]\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472c5f78",
   "metadata": {},
   "source": [
    "## Goal: find x that minimises y, using the fitted model as a surrogate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1f48a4",
   "metadata": {},
   "source": [
    "Available acquisition functions for this goal:\n",
    "\n",
    "    - ModelMin / ModelMax: pure exploitation.  No exploration at all --\n",
    "      just returns the predicted optimum.  Use only when you fully trust\n",
    "      the model.\n",
    "\n",
    "    - LCB (Lower Confidence Bound): y_hat - kappa*sigma.  The kappa\n",
    "      parameter controls exploration vs exploitation:\n",
    "        kappa=0  -> pure exploitation (ModelMin)\n",
    "        kappa~2  -> balanced\n",
    "        kappa>3  -> heavy exploration\n",
    "\n",
    "    - UCB (Upper Confidence Bound): the mirror image for maximisation.\n",
    "\n",
    "    - ExpectedImprovement (EI): the Bayesian optimisation gold standard.\n",
    "      Naturally balances exploration and exploitation without a tuning\n",
    "      parameter (just xi, which is usually small).  Recommended as the\n",
    "      default for optimisation.\n",
    "\n",
    "    - ProbabilityOfImprovement (PI): similar to EI but only cares about\n",
    "      the *probability* of beating the current best, not the magnitude\n",
    "      of improvement.  More exploitative than EI for the same xi.\n",
    "\n",
    "    - ThompsonSampling: draws a random model from the posterior and\n",
    "      optimises that.  Produces diverse batches naturally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea750922",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T22:37:18.925815Z",
     "iopub.status.busy": "2026-02-10T22:37:18.925739Z",
     "iopub.status.idle": "2026-02-10T22:37:19.153806Z",
     "shell.execute_reply": "2026-02-10T22:37:19.153412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: y = 0.0132*x^3 + 0.8891*x^2 + 1.827 - 2.736*x\n",
      "True minimum at x=1.5 (y = 2.25 - 4.5 + 2 = -0.25)\n",
      "\n",
      "  ModelMin (exploit only)             -> x = [1.40, 1.57, 1.41]\n",
      "  LCB kappa=0.5 (exploitative)        -> x = [1.40, 1.57, 1.41]\n",
      "  LCB kappa=2 (balanced)              -> x = [1.40, 1.41, 1.57]\n",
      "  LCB kappa=5 (exploratory)           -> x = [1.58, 1.41, 1.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Expected Improvement                -> x = [1.40, 1.41, 1.57]\n",
      "  Prob. of Improvement                -> x = [1.40, 1.41, 1.57]\n",
      "  Thompson Sampling                   -> x = [1.58, 1.58, 1.57]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bounds = [(0.0, 5.0)]\n",
    "\n",
    "print(f\"Model: {model.expression_}\")\n",
    "print(\"True minimum at x=1.5 (y = 2.25 - 4.5 + 2 = -0.25)\")\n",
    "print()\n",
    "\n",
    "strategies = [\n",
    "    (\"ModelMin (exploit only)\", ModelMin()),\n",
    "    (\"LCB kappa=0.5 (exploitative)\", LCB(kappa=0.5)),\n",
    "    (\"LCB kappa=2 (balanced)\", LCB(kappa=2.0)),\n",
    "    (\"LCB kappa=5 (exploratory)\", LCB(kappa=5.0)),\n",
    "    (\"Expected Improvement\", ExpectedImprovement(minimize=True)),\n",
    "    (\"Prob. of Improvement\", ProbabilityOfImprovement(minimize=True)),\n",
    "    (\"Thompson Sampling\", ThompsonSampling(minimize=True, seed=42)),\n",
    "]\n",
    "\n",
    "for name, acq in strategies:\n",
    "    result = suggest_points(model, bounds, acq, n_points=3, random_state=42)\n",
    "    pts = np.array(result.points).ravel()\n",
    "    print(f\"  {name:35s} -> x = [{', '.join(f'{p:.2f}' for p in pts)}]\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38d5b3d",
   "metadata": {},
   "source": [
    "## Goal: figure out which model form is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d02c0b",
   "metadata": {},
   "source": [
    "When the Pareto front contains models of different complexities that\n",
    "    all fit the data similarly, you need data points that *discriminate*\n",
    "    between them.\n",
    "\n",
    "    - ModelDiscrimination: scores candidates by the maximum disagreement\n",
    "      among Pareto-front models.\n",
    "\n",
    "    - EnsembleDisagreement: standard deviation across Pareto models.\n",
    "      Similar idea but uses std instead of max-min range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59619ba6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T22:37:19.155312Z",
     "iopub.status.busy": "2026-02-10T22:37:19.155224Z",
     "iopub.status.idle": "2026-02-10T22:37:19.287464Z",
     "shell.execute_reply": "2026-02-10T22:37:19.287052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: y = 0.0132*x^3 + 0.8891*x^2 + 1.827 - 2.736*x\n",
      "Pareto front has 3 models:\n",
      "  complexity=3, BIC=81.8: y = 0.09309*x^3\n",
      "  complexity=5, BIC=63.0: y = 0.1661*x^3 - 0.3433*x^2 + 0.4584\n",
      "  complexity=6, BIC=20.9: y = 0.0132*x^3 + 0.8891*x^2 + 1.827 - 2.736*x\n",
      "\n",
      "  ModelDiscrimination       -> x = [0.02, 0.02, 0.01, 0.01, 0.00]\n",
      "  EnsembleDisagreement      -> x = [0.02, 0.02, 0.01, 0.01, 0.00]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bounds = [(0.0, 5.0)]\n",
    "\n",
    "print(f\"Best model: {model.expression_}\")\n",
    "print(f\"Pareto front has {len(model.pareto_front_)} models:\")\n",
    "for r in model.pareto_front_:\n",
    "    print(f\"  complexity={r.complexity}, BIC={r.bic:.1f}: {r.expression()}\")\n",
    "print()\n",
    "\n",
    "acqs = [\n",
    "    (\"ModelDiscrimination\", ModelDiscrimination()),\n",
    "    (\"EnsembleDisagreement\", EnsembleDisagreement()),\n",
    "]\n",
    "\n",
    "for name, acq in acqs:\n",
    "    result = suggest_points(model, bounds, acq, n_points=5, random_state=42)\n",
    "    pts = np.array(result.points).ravel()\n",
    "    print(f\"  {name:25s} -> x = [{', '.join(f'{p:.2f}' for p in pts)}]\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2921e3c",
   "metadata": {},
   "source": [
    "## Goal: select a *batch* of points that are collectively informative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559a6200",
   "metadata": {},
   "source": [
    "When you select the top-k by acquisition score (greedy), the points\n",
    "    can cluster in one region.  Batch strategies address this:\n",
    "\n",
    "    - greedy: top-k by raw score.  Fast but may cluster.\n",
    "\n",
    "    - penalized: after selecting the best candidate, nearby candidates\n",
    "      are penalised before selecting the next.  Simple diversity.\n",
    "\n",
    "    - kriging_believer: after selecting each point, the model is\n",
    "      temporarily updated with a \"fantasy\" observation (y_hat) and\n",
    "      re-scored.  More sophisticated -- later selections account for\n",
    "      information gained by earlier ones.\n",
    "\n",
    "    - d_optimal: selects the batch that maximises det(Phi^T Phi),\n",
    "      ignoring the acquisition function entirely.  Best for pure\n",
    "      space-filling / information maximisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "876009e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T22:37:19.289071Z",
     "iopub.status.busy": "2026-02-10T22:37:19.288989Z",
     "iopub.status.idle": "2026-02-10T22:37:24.867625Z",
     "shell.execute_reply": "2026-02-10T22:37:24.867121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  greedy               -> x = [0.00, 0.01, 0.01, 4.99, 5.00]  (spread=4.99)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  penalized            -> x = [0.00, 1.40, 3.15, 4.25, 5.00]  (spread=5.00)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  kriging_believer     -> x = [0.00, 0.01, 0.01, 4.99, 5.00]  (spread=5.00)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  d_optimal            -> x = [0.00, 1.36, 3.49, 3.73, 5.00]  (spread=5.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bounds = [(0.0, 5.0)]\n",
    "\n",
    "learner = ActiveLearner(model, bounds, PredictionVariance(), random_state=42)\n",
    "\n",
    "for strategy in [\"greedy\", \"penalized\", \"kriging_believer\", \"d_optimal\"]:\n",
    "    result = learner.suggest(n_points=5, batch_strategy=strategy)\n",
    "    pts = sorted(np.array(result.points).ravel())\n",
    "    spread = pts[-1] - pts[0]\n",
    "    print(\n",
    "        f\"  {strategy:20s} -> x = [{', '.join(f'{p:.2f}' for p in pts)}]\"\n",
    "        f\"  (spread={spread:.2f})\"\n",
    "    )\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2383f54d",
   "metadata": {},
   "source": [
    "## Goal: iteratively improve a model by running experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430c2512",
   "metadata": {},
   "source": [
    "The workflow is:\n",
    "    1. Fit an initial model on a small dataset.\n",
    "    2. Use an acquisition function to suggest new points.\n",
    "    3. \"Run the experiment\" (here: evaluate the true function + noise).\n",
    "    4. Update the model with the new data.\n",
    "    5. Repeat until converged or budget exhausted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e7ee132",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T22:37:24.868865Z",
     "iopub.status.busy": "2026-02-10T22:37:24.868790Z",
     "iopub.status.idle": "2026-02-10T22:37:28.714687Z",
     "shell.execute_reply": "2026-02-10T22:37:28.714320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model (15 points): y = 0.01273*x^3 - 2.784*x + 1.862 + 0.9046*x^2\n",
      "  R^2 = 0.9981\n",
      "  MSE = 0.0236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 1: n=20, R^2=0.9908, MSE=0.1040, model=y = 0.1458*x^3 - 0.2237*x^2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 2: n=25, R^2=0.9902, MSE=0.1002, model=y = 0.1483*x^3 - 0.2344*x^2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 3: n=30, R^2=0.9969, MSE=0.0280, model=y = 0.02827*x^3 + 0.777*x^2 + 1.729 - 2.496*x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 4: n=35, R^2=0.9962, MSE=0.0300, model=y = 0.0279*x^3 + 0.7865*x^2 + 1.756 - 2.538*x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 5: n=40, R^2=0.9957, MSE=0.0292, model=y = 0.01841*x^3 + 0.8644*x^2 + 1.875 - 2.722*x\n",
      "\n",
      "Final model (40 points): y = 0.01841*x^3 + 0.8644*x^2 + 1.875 - 2.722*x\n"
     ]
    }
   ],
   "source": [
    "# True function (unknown to the model)\n",
    "def oracle(X):\n",
    "    X = np.array(X)\n",
    "    return X[:, 0] ** 2 - 3.0 * X[:, 0] + 2.0 + np.random.randn(len(X)) * 0.2\n",
    "\n",
    "# Start with very few points\n",
    "np.random.seed(0)\n",
    "X_init = np.random.uniform(0, 5, (15, 1))\n",
    "y_init = oracle(X_init)\n",
    "\n",
    "library = (\n",
    "    BasisLibrary(n_features=1, feature_names=[\"x\"])\n",
    "    .add_constant()\n",
    "    .add_linear()\n",
    "    .add_polynomials(max_degree=3)\n",
    ")\n",
    "model = SymbolicRegressor(basis_library=library, max_terms=4, strategy=\"greedy_forward\")\n",
    "model.fit(jnp.array(X_init), jnp.array(y_init))\n",
    "\n",
    "print(f\"Initial model ({len(y_init)} points): {model.expression_}\")\n",
    "print(f\"  R^2 = {model.score(model._X_train, model._y_train):.4f}\")\n",
    "print(f\"  MSE = {model.metrics_['mse']:.4f}\")\n",
    "\n",
    "# Active learning loop\n",
    "learner = ActiveLearner(\n",
    "    model,\n",
    "    bounds=[(0.0, 5.0)],\n",
    "    acquisition=ExpectedImprovement(minimize=True),\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "n_iterations = 5\n",
    "points_per_iteration = 5\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    result = learner.suggest(\n",
    "        n_points=points_per_iteration,\n",
    "        batch_strategy=\"penalized\",\n",
    "    )\n",
    "\n",
    "    y_new = oracle(np.array(result.points))\n",
    "    learner.update(result.points, jnp.array(y_new))\n",
    "\n",
    "    print(\n",
    "        f\"  Iteration {i + 1}: \"\n",
    "        f\"n={learner.n_observations}, \"\n",
    "        f\"R^2={model.score(model._X_train, model._y_train):.4f}, \"\n",
    "        f\"MSE={model.metrics_['mse']:.4f}, \"\n",
    "        f\"model={model.expression_}\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nFinal model ({learner.n_observations} points): {model.expression_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce1b01b",
   "metadata": {},
   "source": [
    "## Goal: combine multiple objectives using weighted acquisition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9394eaf9",
   "metadata": {},
   "source": [
    "You can weight and add acquisition functions together to balance\n",
    "    different goals simultaneously.  Each component is min-max normalised\n",
    "    before weighting so the weights are meaningful.\n",
    "\n",
    "    Common recipes:\n",
    "    - Balanced optimisation:  0.7 * EI + 0.3 * PredictionVariance\n",
    "    - Exploration with model improvement:  0.5 * PredictionVariance + 0.5 * AOptimal\n",
    "    - Multi-objective:  0.4 * ModelMin + 0.3 * PredictionVariance + 0.3 * DOptimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "256a64d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T22:37:28.716133Z",
     "iopub.status.busy": "2026-02-10T22:37:28.716056Z",
     "iopub.status.idle": "2026-02-10T22:37:29.304460Z",
     "shell.execute_reply": "2026-02-10T22:37:29.303989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.7*EI + 0.3*Variance          -> x = [4.99, 4.99, 5.00]\n",
      "  0.5*LCB + 0.5*DOptimal         -> x = [1.23, 1.24, 1.24]\n",
      "  Equal: EI + Var + AOptimal     -> x = [4.99, 4.99, 5.00]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bounds = [(0.0, 5.0)]\n",
    "\n",
    "composites = [\n",
    "    (\n",
    "        \"0.7*EI + 0.3*Variance\",\n",
    "        0.7 * ExpectedImprovement(minimize=True) + 0.3 * PredictionVariance(),\n",
    "    ),\n",
    "    (\n",
    "        \"0.5*LCB + 0.5*DOptimal\",\n",
    "        0.5 * LCB(kappa=2) + 0.5 * DOptimal(),\n",
    "    ),\n",
    "    (\n",
    "        \"Equal: EI + Var + AOptimal\",\n",
    "        ExpectedImprovement(minimize=True) + PredictionVariance() + AOptimal(),\n",
    "    ),\n",
    "]\n",
    "\n",
    "for name, acq in composites:\n",
    "    result = suggest_points(model, bounds, acq, n_points=3, random_state=42)\n",
    "    pts = np.array(result.points).ravel()\n",
    "    print(f\"  {name:30s} -> x = [{', '.join(f'{p:.2f}' for p in pts)}]\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2943a0a6",
   "metadata": {},
   "source": [
    "## Decision Guide: Choosing an Acquisition Function\n",
    "\n",
    "**WHAT IS YOUR GOAL?**\n",
    "\n",
    "1. **IMPROVE MODEL ACCURACY** (explore everywhere)\n",
    "   - Simple & fast? → `PredictionVariance`\n",
    "   - Need coverage guarantee? → `ConfidenceBandWidth(alpha=0.05)`\n",
    "   - Unsure about model form? → `EnsembleDisagreement` or `BMAUncertainty`\n",
    "   - Tighten coefficient CIs? → `AOptimal`\n",
    "   - Max info per experiment? → `DOptimal`\n",
    "\n",
    "2. **FIND THE OPTIMUM** (minimise or maximise y)\n",
    "   - Trust the model fully? → `ModelMin` / `ModelMax`\n",
    "   - Want balanced exploration? → `ExpectedImprovement` (recommended)\n",
    "   - Need probability of beating a threshold? → `ProbabilityOfImprovement`\n",
    "   - Want explicit exploration knob? → `LCB(kappa)` / `UCB(kappa)`\n",
    "   - Want randomised exploration? → `ThompsonSampling`\n",
    "\n",
    "3. **DECIDE WHICH MODEL IS CORRECT**\n",
    "   - Pareto models disagree? → `ModelDiscrimination`\n",
    "   - Quantify structural uncertainty? → `EnsembleDisagreement`\n",
    "\n",
    "4. **MULTIPLE OBJECTIVES**\n",
    "   - Combine with weights: `0.7 * EI + 0.3 * PredictionVariance`\n",
    "\n",
    "**BATCH STRATEGY SELECTION:**\n",
    "- Fast, don't care about diversity? → `greedy`\n",
    "- Want spatial diversity? → `penalized`\n",
    "- Want information-aware batches? → `kriging_believer`\n",
    "- Want maximum design efficiency? → `d_optimal`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
