{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison: Langmuir vs Freundlich Isotherms\n",
    "\n",
    "When experimental data could plausibly follow more than one known model, **JAXSR** can\n",
    "help you decide which model best explains the data. This notebook demonstrates a\n",
    "systematic comparison of two classic adsorption isotherms:\n",
    "\n",
    "| Model | Equation | Linear parameters | Nonlinear parameters |\n",
    "|-------|----------|-------------------|---------------------|\n",
    "| **Langmuir** | $q = q_{\\max} \\frac{K P}{1 + K P}$ | $q_{\\max}$ | $K$ |\n",
    "| **Freundlich** | $q = K_f P^{1/n}$ | $K_f$ | $n$ |\n",
    "\n",
    "Both models encode domain knowledge via **parametric basis functions**: the nonlinear\n",
    "parameter is optimized by profile likelihood, while the linear coefficient is estimated\n",
    "by OLS.\n",
    "\n",
    "**What you will learn:**\n",
    "1. How to encode competing hypotheses as separate `BasisLibrary` objects\n",
    "2. How to compare fitted models using information criteria (AIC, BIC, AICc)\n",
    "3. How to use `compare_models()` for a structured side-by-side comparison\n",
    "4. How to use cross-validation and bootstrap analysis for robustness\n",
    "5. How to use ANOVA to assess term significance within each model\n",
    "6. How to visualize competing fits with prediction intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Adsorption Data\n",
    "\n",
    "We simulate data from a **true Langmuir** isotherm with $q_{\\max}=5.0$ and $K=2.0$,\n",
    "then add realistic measurement noise. The goal is to see whether our comparison\n",
    "framework correctly identifies Langmuir as the better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# True Langmuir parameters\n",
    "Q_MAX_TRUE = 5.0\n",
    "K_TRUE = 2.0\n",
    "\n",
    "# Experimental pressures (log-spaced for better coverage of the curve shape)\n",
    "n_obs = 30\n",
    "P = np.sort(np.random.uniform(0.05, 12.0, n_obs))\n",
    "X = P.reshape(-1, 1)\n",
    "\n",
    "# True response + heteroscedastic noise (larger noise at higher loadings)\n",
    "q_true = Q_MAX_TRUE * K_TRUE * P / (1 + K_TRUE * P)\n",
    "noise_std = 0.10 + 0.02 * q_true  # ~2-12% relative noise\n",
    "q_measured = q_true + noise_std * np.random.randn(n_obs)\n",
    "\n",
    "print(f\"Observations: {n_obs}\")\n",
    "print(f\"Pressure range: [{P.min():.2f}, {P.max():.2f}] bar\")\n",
    "print(f\"Loading range:  [{q_measured.min():.2f}, {q_measured.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / test split\n",
    "\n",
    "We hold out 20% of the data for out-of-sample comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, q_measured, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training: {len(X_train)},  Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Competing Models as Basis Libraries\n",
    "\n",
    "Each isotherm becomes a `BasisLibrary` with:\n",
    "- A **constant** term (intercept — should be near zero for a pure isotherm)\n",
    "- A **parametric** basis function encoding the nonlinear parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxsr import BasisLibrary\n",
    "\n",
    "# --- Langmuir basis ---\n",
    "# q = q_max * K*P / (1 + K*P)\n",
    "# Linear parameter: q_max (OLS coefficient)\n",
    "# Nonlinear parameter: K (profile-likelihood)\n",
    "langmuir_library = (\n",
    "    BasisLibrary(n_features=1, feature_names=[\"P\"])\n",
    "    .add_constant()\n",
    "    .add_parametric(\n",
    "        name=\"K*P/(1+K*P)\",\n",
    "        func=lambda X, K: K * X[:, 0] / (1 + K * X[:, 0]),\n",
    "        param_bounds={\"K\": (0.01, 100.0)},\n",
    "        complexity=3,\n",
    "        feature_indices=(0,),\n",
    "        log_scale=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- Freundlich basis ---\n",
    "# q = K_f * P^(1/n)\n",
    "# Linear parameter: K_f (OLS coefficient)\n",
    "# Nonlinear parameter: n (profile-likelihood)\n",
    "freundlich_library = (\n",
    "    BasisLibrary(n_features=1, feature_names=[\"P\"])\n",
    "    .add_constant()\n",
    "    .add_parametric(\n",
    "        name=\"P^(1/n)\",\n",
    "        func=lambda X, n: X[:, 0] ** (1.0 / n),\n",
    "        param_bounds={\"n\": (0.5, 20.0)},\n",
    "        complexity=3,\n",
    "        feature_indices=(0,),\n",
    "        log_scale=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Langmuir library:  {len(langmuir_library)} basis functions\")\n",
    "print(f\"Freundlich library: {len(freundlich_library)} basis functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fit Both Models\n",
    "\n",
    "We use `SymbolicRegressor` with `max_terms=2` and `information_criterion=\"aicc\"`\n",
    "(corrected AIC, appropriate for small-sample situations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxsr import SymbolicRegressor\n",
    "\n",
    "model_langmuir = SymbolicRegressor(\n",
    "    basis_library=langmuir_library,\n",
    "    max_terms=2,\n",
    "    strategy=\"greedy_forward\",\n",
    "    information_criterion=\"aicc\",\n",
    ")\n",
    "model_langmuir.fit(X_train, y_train)\n",
    "\n",
    "model_freundlich = SymbolicRegressor(\n",
    "    basis_library=freundlich_library,\n",
    "    max_terms=2,\n",
    "    strategy=\"greedy_forward\",\n",
    "    information_criterion=\"aicc\",\n",
    ")\n",
    "model_freundlich.fit(X_train, y_train)\n",
    "\n",
    "print(\"=== Langmuir ===\")\n",
    "print(model_langmuir.summary())\n",
    "print(f\"\\nExpression: {model_langmuir.expression_}\")\n",
    "\n",
    "print(\"\\n=== Freundlich ===\")\n",
    "print(model_freundlich.summary())\n",
    "print(f\"\\nExpression: {model_freundlich.expression_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Information Criteria Comparison\n",
    "\n",
    "Lower is better for AIC, BIC, and AICc. The model with the lowest criterion value\n",
    "best balances goodness-of-fit against complexity.\n",
    "\n",
    "**Rules of thumb:**\n",
    "- $\\Delta \\text{IC} < 2$: Models are essentially indistinguishable\n",
    "- $2 < \\Delta \\text{IC} < 10$: Moderate evidence for the better model\n",
    "- $\\Delta \\text{IC} > 10$: Strong evidence for the better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"Langmuir\": model_langmuir, \"Freundlich\": model_freundlich}\n",
    "\n",
    "print(f\"{'Model':<14s} {'R²':>8s} {'MSE':>10s} {'AIC':>8s} {'BIC':>8s} {'AICc':>8s}\")\n",
    "print(\"-\" * 62)\n",
    "for name, m in models.items():\n",
    "    met = m.metrics_\n",
    "    print(\n",
    "        f\"{name:<14s} {met['r2']:8.5f} {met['mse']:10.6f} \"\n",
    "        f\"{met['aic']:8.2f} {met['bic']:8.2f} {met['aicc']:8.2f}\"\n",
    "    )\n",
    "\n",
    "# Delta AICc\n",
    "aicc_lang = model_langmuir.metrics_[\"aicc\"]\n",
    "aicc_freu = model_freundlich.metrics_[\"aicc\"]\n",
    "delta = abs(aicc_lang - aicc_freu)\n",
    "winner = \"Langmuir\" if aicc_lang < aicc_freu else \"Freundlich\"\n",
    "\n",
    "print(f\"\\nΔAICc = {delta:.2f}  →  {winner} is preferred\")\n",
    "if delta < 2:\n",
    "    print(\"  (weak evidence — models are nearly equivalent)\")\n",
    "elif delta < 10:\n",
    "    print(\"  (moderate evidence)\")\n",
    "else:\n",
    "    print(\"  (strong evidence)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Structured Comparison with `compare_models()`\n",
    "\n",
    "JAXSR provides `compare_models()` for an automated side-by-side evaluation including\n",
    "both training and held-out test metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxsr import compare_models, format_comparison_table\n",
    "\n",
    "comparison = compare_models(\n",
    "    models=[model_langmuir, model_freundlich],\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    names=[\"Langmuir\", \"Freundlich\"],\n",
    ")\n",
    "\n",
    "print(format_comparison_table(comparison))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross-Validation\n",
    "\n",
    "Information criteria are asymptotic approximations. Cross-validation gives a\n",
    "non-parametric estimate of out-of-sample performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxsr import cross_validate\n",
    "\n",
    "cv_lang = cross_validate(\n",
    "    model_langmuir, X_train, y_train, cv=5, scoring=\"neg_mse\", random_state=42\n",
    ")\n",
    "cv_freu = cross_validate(\n",
    "    model_freundlich, X_train, y_train, cv=5, scoring=\"neg_mse\", random_state=42\n",
    ")\n",
    "\n",
    "print(\"5-Fold Cross-Validation (negative MSE, higher is better):\")\n",
    "print(\n",
    "    f\"  Langmuir:   {cv_lang['mean_test_score']:.6f} \"\n",
    "    f\"± {cv_lang['std_test_score']:.6f}\"\n",
    ")\n",
    "print(\n",
    "    f\"  Freundlich: {cv_freu['mean_test_score']:.6f} \"\n",
    "    f\"± {cv_freu['std_test_score']:.6f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ANOVA — Term Significance\n",
    "\n",
    "ANOVA decomposes the explained variation by term. For a good model, the parametric\n",
    "isotherm term should explain nearly all the variance, while the intercept contributes\n",
    "little (it should be near zero for a pure isotherm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxsr import anova\n",
    "\n",
    "print(\"=== Langmuir ANOVA ===\")\n",
    "anova_lang = anova(model_langmuir)\n",
    "summary_sources = {\"Model\", \"Residual\", \"Total\"}\n",
    "term_rows_lang = [r for r in anova_lang.rows if r.source not in summary_sources]\n",
    "total_ss_lang = sum(r.sum_sq for r in term_rows_lang)\n",
    "\n",
    "print(f\"{'Source':<25s} {'SS':>10s} {'%':>7s} {'F':>10s} {'p-value':>10s}\")\n",
    "print(\"-\" * 65)\n",
    "for row in term_rows_lang:\n",
    "    pct = 100 * row.sum_sq / total_ss_lang if total_ss_lang > 0 else 0\n",
    "    sig = \"***\" if row.p_value < 0.001 else \"**\" if row.p_value < 0.01 else \"*\" if row.p_value < 0.05 else \"\"\n",
    "    print(\n",
    "        f\"{row.source:<25s} {row.sum_sq:10.4f} {pct:6.1f}% \"\n",
    "        f\"{row.f_value:10.2f} {row.p_value:10.4f} {sig}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n=== Freundlich ANOVA ===\")\n",
    "anova_freu = anova(model_freundlich)\n",
    "term_rows_freu = [r for r in anova_freu.rows if r.source not in summary_sources]\n",
    "total_ss_freu = sum(r.sum_sq for r in term_rows_freu)\n",
    "\n",
    "print(f\"{'Source':<25s} {'SS':>10s} {'%':>7s} {'F':>10s} {'p-value':>10s}\")\n",
    "print(\"-\" * 65)\n",
    "for row in term_rows_freu:\n",
    "    pct = 100 * row.sum_sq / total_ss_freu if total_ss_freu > 0 else 0\n",
    "    sig = \"***\" if row.p_value < 0.001 else \"**\" if row.p_value < 0.01 else \"*\" if row.p_value < 0.05 else \"\"\n",
    "    print(\n",
    "        f\"{row.source:<25s} {row.sum_sq:10.4f} {pct:6.1f}% \"\n",
    "        f\"{row.f_value:10.2f} {row.p_value:10.4f} {sig}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Bootstrap Stability Analysis\n",
    "\n",
    "Bootstrap resampling tests how sensitive each model's coefficients are to the\n",
    "specific data points. A model with tighter bootstrap intervals is more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxsr import bootstrap_coefficients\n",
    "\n",
    "boot_lang = bootstrap_coefficients(model_langmuir, n_bootstrap=500, alpha=0.05, seed=42)\n",
    "boot_freu = bootstrap_coefficients(model_freundlich, n_bootstrap=500, alpha=0.05, seed=42)\n",
    "\n",
    "print(\"=== Langmuir Bootstrap Coefficients (95% CI) ===\")\n",
    "for name, lo, hi, std in zip(\n",
    "    boot_lang[\"names\"],\n",
    "    boot_lang[\"lower\"],\n",
    "    boot_lang[\"upper\"],\n",
    "    boot_lang[\"std\"],\n",
    "    strict=False,\n",
    "):\n",
    "    print(f\"  {name}: [{float(lo):.4f}, {float(hi):.4f}]  (std={float(std):.4f})\")\n",
    "\n",
    "print(\"\\n=== Freundlich Bootstrap Coefficients (95% CI) ===\")\n",
    "for name, lo, hi, std in zip(\n",
    "    boot_freu[\"names\"],\n",
    "    boot_freu[\"lower\"],\n",
    "    boot_freu[\"upper\"],\n",
    "    boot_freu[\"std\"],\n",
    "    strict=False,\n",
    "):\n",
    "    print(f\"  {name}: [{float(lo):.4f}, {float(hi):.4f}]  (std={float(std):.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visual Comparison\n",
    "\n",
    "Plot both models' fits with 95% prediction intervals alongside the true curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "P_grid = np.linspace(0.01, 13.0, 200).reshape(-1, 1)\n",
    "q_grid_true = Q_MAX_TRUE * K_TRUE * P_grid.flatten() / (1 + K_TRUE * P_grid.flatten())\n",
    "\n",
    "# Predictions and intervals\n",
    "y_lang, lo_lang, hi_lang = model_langmuir.predict_interval(P_grid, alpha=0.05)\n",
    "y_freu, lo_freu, hi_freu = model_freundlich.predict_interval(P_grid, alpha=0.05)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "\n",
    "for ax, name, y_pred, lo, hi, color in [\n",
    "    (axes[0], \"Langmuir\", y_lang, lo_lang, hi_lang, \"steelblue\"),\n",
    "    (axes[1], \"Freundlich\", y_freu, lo_freu, hi_freu, \"darkorange\"),\n",
    "]:\n",
    "    ax.fill_between(\n",
    "        P_grid.flatten(),\n",
    "        np.asarray(lo).flatten(),\n",
    "        np.asarray(hi).flatten(),\n",
    "        alpha=0.2,\n",
    "        color=color,\n",
    "        label=\"95% PI\",\n",
    "    )\n",
    "    ax.plot(P_grid.flatten(), np.asarray(y_pred).flatten(), color=color, lw=2, label=f\"{name} fit\")\n",
    "    ax.plot(P_grid.flatten(), q_grid_true, \"k--\", lw=1.5, label=\"True Langmuir\")\n",
    "    ax.scatter(X_train.flatten(), y_train, c=\"black\", s=30, zorder=5, label=\"Train\")\n",
    "    ax.scatter(X_test.flatten(), y_test, c=\"red\", marker=\"x\", s=50, zorder=5, label=\"Test\")\n",
    "    ax.set_xlabel(\"Pressure (bar)\")\n",
    "    ax.set_title(f\"{name}  (R²={models[name].metrics_['r2']:.4f})\")\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "axes[0].set_ylabel(\"Loading, q\")\n",
    "fig.suptitle(\"Model Comparison: Langmuir vs Freundlich\", fontsize=13, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Residual Diagnostics\n",
    "\n",
    "Good residuals should be:\n",
    "- **Centered at zero** (no systematic bias)\n",
    "- **Homoscedastic** (constant spread across predicted values)\n",
    "- **Normally distributed** (for valid confidence intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, name, m, color in [\n",
    "    (axes[0], \"Langmuir\", model_langmuir, \"steelblue\"),\n",
    "    (axes[1], \"Freundlich\", model_freundlich, \"darkorange\"),\n",
    "]:\n",
    "    y_pred_train = np.asarray(m.predict(X_train))\n",
    "    residuals = np.asarray(y_train) - y_pred_train.flatten()\n",
    "\n",
    "    ax.scatter(y_pred_train.flatten(), residuals, c=color, s=30, alpha=0.7)\n",
    "    ax.axhline(0, color=\"k\", ls=\"--\", lw=1)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Residual\")\n",
    "    ax.set_title(f\"{name} Residuals\")\n",
    "\n",
    "    # Add std bands\n",
    "    res_std = np.std(residuals)\n",
    "    ax.axhline(2 * res_std, color=\"gray\", ls=\":\", lw=0.8)\n",
    "    ax.axhline(-2 * res_std, color=\"gray\", ls=\":\", lw=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Out-of-Sample Test Metrics\n",
    "\n",
    "The ultimate test: how well does each model predict data it has never seen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxsr import compute_mae, compute_mse, compute_r2\n",
    "\n",
    "print(f\"{'Metric':<12s} {'Langmuir':>12s} {'Freundlich':>12s} {'Winner':>12s}\")\n",
    "print(\"-\" * 52)\n",
    "\n",
    "for metric_name, metric_fn, higher_better in [\n",
    "    (\"R²\", compute_r2, True),\n",
    "    (\"MSE\", compute_mse, False),\n",
    "    (\"MAE\", compute_mae, False),\n",
    "]:\n",
    "    val_l = float(metric_fn(y_test, model_langmuir.predict(X_test)))\n",
    "    val_f = float(metric_fn(y_test, model_freundlich.predict(X_test)))\n",
    "\n",
    "    if higher_better:\n",
    "        best = \"Langmuir\" if val_l >= val_f else \"Freundlich\"\n",
    "    else:\n",
    "        best = \"Langmuir\" if val_l <= val_f else \"Freundlich\"\n",
    "\n",
    "    print(f\"{metric_name:<12s} {val_l:12.6f} {val_f:12.6f} {best:>12s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Parameter Extraction\n",
    "\n",
    "For the winning model, extract the physical parameters with uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine winner by AICc\n",
    "best_name = \"Langmuir\" if aicc_lang < aicc_freu else \"Freundlich\"\n",
    "best_model = models[best_name]\n",
    "\n",
    "print(f\"Selected model: {best_name}\")\n",
    "print(f\"Expression: {best_model.expression_}\")\n",
    "print(f\"LaTeX: ${best_model.to_latex()}$\")\n",
    "\n",
    "print(\"\\n--- Coefficient Estimates (95% OLS intervals) ---\")\n",
    "intervals = best_model.coefficient_intervals(alpha=0.05)\n",
    "for name, (est, lo, hi, se) in intervals.items():\n",
    "    sig = \" *\" if lo * hi > 0 else \"\"\n",
    "    print(f\"  {name}: {est:.4f}  [{lo:.4f}, {hi:.4f}]  SE={se:.4f}{sig}\")\n",
    "\n",
    "if best_name == \"Langmuir\":\n",
    "    print(\"\\n--- Physical Parameters ---\")\n",
    "    # q_max is the coefficient of the K*P/(1+K*P) term\n",
    "    for i, fname in enumerate(best_model.selected_features_):\n",
    "        if \"K*P\" in fname:\n",
    "            q_max_est = float(best_model.coefficients_[i])\n",
    "            print(f\"  q_max (estimated): {q_max_est:.4f}  (true: {Q_MAX_TRUE})\")\n",
    "    print(\"  K is embedded in the parametric basis (optimized by profile likelihood)\")\n",
    "    print(f\"  True K = {K_TRUE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary and Decision Framework\n",
    "\n",
    "Here is a general framework for comparing competing models with JAXSR:\n",
    "\n",
    "| Step | Method | What it tells you |\n",
    "|------|--------|-------------------|\n",
    "| 1 | Fit both models | Get expressions and coefficients |\n",
    "| 2 | Compare AIC/BIC/AICc | Penalized likelihood ranking |\n",
    "| 3 | `compare_models()` | Structured train+test comparison |\n",
    "| 4 | Cross-validation | Non-parametric out-of-sample estimate |\n",
    "| 5 | ANOVA | Term significance within each model |\n",
    "| 6 | Bootstrap | Coefficient stability under resampling |\n",
    "| 7 | Residual plots | Visual diagnostic for model adequacy |\n",
    "| 8 | Held-out test | Final out-of-sample validation |\n",
    "\n",
    "**Key rules:**\n",
    "- If $\\Delta$AICc < 2, the models are practically equivalent — prefer the simpler one\n",
    "- Cross-validation and test-set metrics should agree with IC rankings\n",
    "- If they disagree, investigate residual patterns for model misspecification\n",
    "- Always check that the intercept is physically sensible (near zero for isotherms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Final Report ===\")\n",
    "print(f\"Best model: {best_name}\")\n",
    "print(f\"Expression: {best_model.expression_}\")\n",
    "print(f\"Train R²: {best_model.metrics_['r2']:.6f}\")\n",
    "print(f\"Test  R²: {float(compute_r2(y_test, best_model.predict(X_test))):.6f}\")\n",
    "print(f\"AICc: {best_model.metrics_['aicc']:.2f}\")\n",
    "print(f\"\\nΔAICc between models: {delta:.2f}\")\n",
    "print(f\"CV MSE (Langmuir):   {-cv_lang['mean_test_score']:.6f}\")\n",
    "print(f\"CV MSE (Freundlich): {-cv_freu['mean_test_score']:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}