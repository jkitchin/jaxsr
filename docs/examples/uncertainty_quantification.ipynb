{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty Quantification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c7c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Uncertainty Quantification Example for JAXSR.\n",
    "\n",
    "Demonstrates the full range of UQ capabilities:\n",
    "1. Classical OLS intervals (coefficient CIs, prediction/confidence bands)\n",
    "2. Pareto front ensemble predictions\n",
    "3. Bayesian Model Averaging\n",
    "4. Conformal prediction (split and jackknife+)\n",
    "5. Residual bootstrap\n",
    "6. UQ visualization\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86666c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09686037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxsr import (\n",
    "    BasisLibrary,\n",
    "    BayesianModelAverage,\n",
    "    SymbolicRegressor,\n",
    "    bootstrap_coefficients,\n",
    "    bootstrap_predict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721979c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_classical_intervals():\n",
    "    \"\"\"\n",
    "    Classical OLS prediction and confidence intervals.\n",
    "\n",
    "    Since JAXSR models are linear-in-parameters (y = Phi @ beta),\n",
    "    standard OLS inference applies directly:\n",
    "      - Cov(beta) = s^2 * (Phi^T Phi)^{-1}\n",
    "      - Prediction variance = s^2 * (1 + h(x_new))\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Example 1: Classical OLS Intervals\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Generate known model: y = 2*x + 1 + noise\n",
    "    np.random.seed(42)\n",
    "    n = 100\n",
    "    X = np.random.uniform(0, 5, (n, 1))\n",
    "    y_true = 2.0 * X[:, 0] + 1.0\n",
    "    y = y_true + np.random.randn(n) * 0.5\n",
    "\n",
    "    X_jax = jnp.array(X)\n",
    "    y_jax = jnp.array(y)\n",
    "\n",
    "    # Fit model\n",
    "    library = (\n",
    "        BasisLibrary(n_features=1, feature_names=[\"x\"])\n",
    "        .add_constant()\n",
    "        .add_linear()\n",
    "        .add_polynomials(max_degree=3)\n",
    "    )\n",
    "\n",
    "    model = SymbolicRegressor(\n",
    "        basis_library=library,\n",
    "        max_terms=3,\n",
    "        strategy=\"greedy_forward\",\n",
    "    )\n",
    "    model.fit(X_jax, y_jax)\n",
    "\n",
    "    print(\"\\nTrue model: y = 2*x + 1 (noise std = 0.5)\")\n",
    "    print(f\"Discovered: {model.expression_}\")\n",
    "\n",
    "    # Noise estimate\n",
    "    print(f\"\\nEstimated noise (sigma): {model.sigma_:.4f} (true: 0.5)\")\n",
    "\n",
    "    # Coefficient intervals\n",
    "    print(\"\\nCoefficient 95% confidence intervals:\")\n",
    "    intervals = model.coefficient_intervals(alpha=0.05)\n",
    "    for name, (est, lo, hi, se) in intervals.items():\n",
    "        print(f\"  {name}: {est:.4f} [{lo:.4f}, {hi:.4f}]  (SE={se:.4f})\")\n",
    "\n",
    "    # Prediction intervals on new data\n",
    "    X_new = jnp.linspace(0, 5, 5).reshape(-1, 1)\n",
    "    y_pred, pred_lo, pred_hi = model.predict_interval(X_new, alpha=0.05)\n",
    "    y_pred_c, conf_lo, conf_hi = model.confidence_band(X_new, alpha=0.05)\n",
    "\n",
    "    print(\"\\nPrediction and confidence intervals at selected points:\")\n",
    "    print(\n",
    "        f\"  {'x':>5}  {'y_pred':>8}  {'pred_lo':>8}  {'pred_hi':>8}  {'conf_lo':>8}  {'conf_hi':>8}\"\n",
    "    )\n",
    "    for i in range(len(X_new)):\n",
    "        print(\n",
    "            f\"  {float(X_new[i, 0]):5.1f}  \"\n",
    "            f\"{float(y_pred[i]):8.3f}  \"\n",
    "            f\"{float(pred_lo[i]):8.3f}  \"\n",
    "            f\"{float(pred_hi[i]):8.3f}  \"\n",
    "            f\"{float(conf_lo[i]):8.3f}  \"\n",
    "            f\"{float(conf_hi[i]):8.3f}\"\n",
    "        )\n",
    "\n",
    "    print(\"\\nNote: Confidence band is narrower \u2014 it estimates E[y|x], not a new y.\")\n",
    "\n",
    "    # Covariance matrix\n",
    "    cov = model.covariance_matrix_\n",
    "    print(\"\\nCoefficient covariance matrix:\")\n",
    "    print(f\"  {np.array(cov)}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f77c2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_ensemble_predictions():\n",
    "    \"\"\"\n",
    "    Pareto front ensemble predictions.\n",
    "\n",
    "    Measures structural/model uncertainty: how much do predictions\n",
    "    vary across plausible model complexities?\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Example 2: Pareto Front Ensemble\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    np.random.seed(42)\n",
    "    X = np.random.uniform(-2, 3, (100, 1))\n",
    "    y = 1.5 * X[:, 0] ** 2 - 0.5 * X[:, 0] + 2.0 + np.random.randn(100) * 0.5\n",
    "\n",
    "    library = (\n",
    "        BasisLibrary(n_features=1, feature_names=[\"x\"])\n",
    "        .add_constant()\n",
    "        .add_linear()\n",
    "        .add_polynomials(max_degree=4)\n",
    "    )\n",
    "\n",
    "    model = SymbolicRegressor(\n",
    "        basis_library=library,\n",
    "        max_terms=5,\n",
    "        strategy=\"greedy_forward\",\n",
    "    )\n",
    "    model.fit(jnp.array(X), jnp.array(y))\n",
    "\n",
    "    print(f\"\\nBest model: {model.expression_}\")\n",
    "\n",
    "    # Pareto front models\n",
    "    print(\"\\nPareto front models:\")\n",
    "    for r in model.pareto_front_:\n",
    "        print(f\"  Complexity {r.complexity}: {r.expression()}\")\n",
    "\n",
    "    # Ensemble predictions\n",
    "    X_new = jnp.linspace(-2, 3, 5).reshape(-1, 1)\n",
    "    result = model.predict_ensemble(X_new)\n",
    "\n",
    "    print(\"\\nEnsemble predictions at selected points:\")\n",
    "    print(f\"  {'x':>5}  {'mean':>8}  {'std':>8}  {'min':>8}  {'max':>8}\")\n",
    "    for i in range(len(X_new)):\n",
    "        print(\n",
    "            f\"  {float(X_new[i, 0]):5.1f}  \"\n",
    "            f\"{float(result['y_mean'][i]):8.3f}  \"\n",
    "            f\"{float(result['y_std'][i]):8.3f}  \"\n",
    "            f\"{float(result['y_min'][i]):8.3f}  \"\n",
    "            f\"{float(result['y_max'][i]):8.3f}\"\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99730cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_bayesian_model_averaging():\n",
    "    \"\"\"\n",
    "    Bayesian Model Averaging (BMA).\n",
    "\n",
    "    Weights models by their BIC/AIC: w_k = exp(-0.5*IC_k) / Z.\n",
    "    BMA variance includes both within-model and between-model components.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Example 3: Bayesian Model Averaging\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    np.random.seed(42)\n",
    "    X = np.random.uniform(0, 5, (120, 1))\n",
    "    y = 2.0 * X[:, 0] + 1.0 + np.random.randn(120) * 0.5\n",
    "\n",
    "    library = (\n",
    "        BasisLibrary(n_features=1, feature_names=[\"x\"])\n",
    "        .add_constant()\n",
    "        .add_linear()\n",
    "        .add_polynomials(max_degree=3)\n",
    "    )\n",
    "\n",
    "    model = SymbolicRegressor(\n",
    "        basis_library=library,\n",
    "        max_terms=4,\n",
    "        strategy=\"greedy_forward\",\n",
    "    )\n",
    "    model.fit(jnp.array(X), jnp.array(y))\n",
    "\n",
    "    # Create BMA\n",
    "    bma = BayesianModelAverage(model, criterion=\"bic\")\n",
    "\n",
    "    print(\"\\nBMA model weights (BIC-based):\")\n",
    "    for expr, weight in bma.weights.items():\n",
    "        print(f\"  {weight:.4f}  {expr}\")\n",
    "\n",
    "    # BMA predictions\n",
    "    X_new = jnp.linspace(0, 5, 5).reshape(-1, 1)\n",
    "    y_mean, y_std = bma.predict(X_new)\n",
    "\n",
    "    print(\"\\nBMA predictions:\")\n",
    "    print(f\"  {'x':>5}  {'mean':>8}  {'std':>8}\")\n",
    "    for i in range(len(X_new)):\n",
    "        print(f\"  {float(X_new[i, 0]):5.1f}  {float(y_mean[i]):8.3f}  {float(y_std[i]):8.3f}\")\n",
    "\n",
    "    # Convenience method with intervals\n",
    "    y_pred, lower, upper = model.predict_bma(X_new, criterion=\"bic\", alpha=0.05)\n",
    "    print(\"\\n95% BMA prediction intervals:\")\n",
    "    print(f\"  {'x':>5}  {'pred':>8}  {'lower':>8}  {'upper':>8}\")\n",
    "    for i in range(len(X_new)):\n",
    "        print(\n",
    "            f\"  {float(X_new[i, 0]):5.1f}  \"\n",
    "            f\"{float(y_pred[i]):8.3f}  \"\n",
    "            f\"{float(lower[i]):8.3f}  \"\n",
    "            f\"{float(upper[i]):8.3f}\"\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306fb8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_conformal_prediction():\n",
    "    \"\"\"\n",
    "    Conformal prediction: distribution-free intervals.\n",
    "\n",
    "    - Split conformal: uses held-out calibration set.\n",
    "    - Jackknife+: uses LOO residuals from training data.\n",
    "\n",
    "    Both provide finite-sample coverage guarantees.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Example 4: Conformal Prediction\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    np.random.seed(42)\n",
    "    n = 300\n",
    "    X_all = np.random.uniform(0, 5, (n, 1))\n",
    "    y_all = 2.0 * X_all[:, 0] + 1.0 + np.random.randn(n) * 0.5\n",
    "\n",
    "    # Split into train / calibration / test\n",
    "    X_train, y_train = jnp.array(X_all[:150]), jnp.array(y_all[:150])\n",
    "    X_cal, y_cal = jnp.array(X_all[150:250]), jnp.array(y_all[150:250])\n",
    "    X_test, y_test = jnp.array(X_all[250:]), jnp.array(y_all[250:])\n",
    "\n",
    "    library = BasisLibrary(n_features=1, feature_names=[\"x\"]).add_constant().add_linear()\n",
    "\n",
    "    model = SymbolicRegressor(\n",
    "        basis_library=library,\n",
    "        max_terms=2,\n",
    "        strategy=\"greedy_forward\",\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"\\nModel: {model.expression_}\")\n",
    "\n",
    "    # Split conformal\n",
    "    y_pred, lower, upper = model.predict_conformal(\n",
    "        X_test, alpha=0.10, method=\"split\", X_cal=X_cal, y_cal=y_cal\n",
    "    )\n",
    "    covered = (y_test >= lower) & (y_test <= upper)\n",
    "    coverage = float(jnp.mean(covered))\n",
    "    print(\"\\nSplit conformal (target 90% coverage):\")\n",
    "    print(f\"  Actual coverage: {coverage:.1%}\")\n",
    "    print(f\"  Avg interval width: {float(jnp.mean(upper - lower)):.3f}\")\n",
    "\n",
    "    # Jackknife+\n",
    "    y_pred_j, lower_j, upper_j = model.predict_conformal(X_test, alpha=0.10, method=\"jackknife+\")\n",
    "    covered_j = (y_test >= lower_j) & (y_test <= upper_j)\n",
    "    coverage_j = float(jnp.mean(covered_j))\n",
    "    print(\"\\nJackknife+ (target 90% coverage):\")\n",
    "    print(f\"  Actual coverage: {coverage_j:.1%}\")\n",
    "    print(f\"  Avg interval width: {float(jnp.mean(upper_j - lower_j)):.3f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a85132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_bootstrap():\n",
    "    \"\"\"\n",
    "    Residual bootstrap: no Gaussian assumption needed.\n",
    "\n",
    "    Resamples residuals to create y* = y_hat + e*, then refits.\n",
    "    Vectorized with NumPy for efficiency.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Example 5: Residual Bootstrap\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    np.random.seed(42)\n",
    "    X = np.random.uniform(0, 5, (100, 1))\n",
    "    y = 2.0 * X[:, 0] + 1.0 + np.random.randn(100) * 0.5\n",
    "\n",
    "    library = BasisLibrary(n_features=1, feature_names=[\"x\"]).add_constant().add_linear()\n",
    "\n",
    "    model = SymbolicRegressor(\n",
    "        basis_library=library,\n",
    "        max_terms=2,\n",
    "        strategy=\"greedy_forward\",\n",
    "    )\n",
    "    model.fit(jnp.array(X), jnp.array(y))\n",
    "\n",
    "    print(f\"\\nModel: {model.expression_}\")\n",
    "\n",
    "    # Bootstrap coefficient CIs\n",
    "    result = bootstrap_coefficients(model, n_bootstrap=2000, alpha=0.05, seed=42)\n",
    "    print(\"\\nBootstrap 95% coefficient CIs (B=2000):\")\n",
    "    for i, name in enumerate(result[\"names\"]):\n",
    "        print(\n",
    "            f\"  {name}: {float(result['mean'][i]):.4f} \"\n",
    "            f\"[{float(result['lower'][i]):.4f}, {float(result['upper'][i]):.4f}]  \"\n",
    "            f\"(std={float(result['std'][i]):.4f})\"\n",
    "        )\n",
    "\n",
    "    # Bootstrap prediction intervals\n",
    "    X_new = jnp.linspace(0, 5, 5).reshape(-1, 1)\n",
    "    pred_result = bootstrap_predict(model, X_new, n_bootstrap=2000, alpha=0.05, seed=42)\n",
    "\n",
    "    print(\"\\nBootstrap 95% prediction intervals:\")\n",
    "    print(f\"  {'x':>5}  {'pred':>8}  {'lower':>8}  {'upper':>8}\")\n",
    "    for i in range(len(X_new)):\n",
    "        print(\n",
    "            f\"  {float(X_new[i, 0]):5.1f}  \"\n",
    "            f\"{float(pred_result['y_pred'][i]):8.3f}  \"\n",
    "            f\"{float(pred_result['lower'][i]):8.3f}  \"\n",
    "            f\"{float(pred_result['upper'][i]):8.3f}\"\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb0d8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_visualization():\n",
    "    \"\"\"\n",
    "    UQ visualization: fan charts, forest plots, BMA weights.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Example 6: UQ Visualization\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        import matplotlib\n",
    "\n",
    "        matplotlib.use(\"Agg\")  # Non-interactive backend\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        from jaxsr.plotting import (\n",
    "            plot_bma_weights,\n",
    "            plot_coefficient_intervals,\n",
    "            plot_prediction_intervals,\n",
    "        )\n",
    "    except ImportError:\n",
    "        print(\"matplotlib not available, skipping visualization example\")\n",
    "        return None\n",
    "\n",
    "    np.random.seed(42)\n",
    "    X = np.random.uniform(0, 5, (100, 1))\n",
    "    y = 2.0 * X[:, 0] + 1.0 + np.random.randn(100) * 0.5\n",
    "\n",
    "    library = (\n",
    "        BasisLibrary(n_features=1, feature_names=[\"x\"])\n",
    "        .add_constant()\n",
    "        .add_linear()\n",
    "        .add_polynomials(max_degree=3)\n",
    "    )\n",
    "\n",
    "    model = SymbolicRegressor(\n",
    "        basis_library=library,\n",
    "        max_terms=4,\n",
    "        strategy=\"greedy_forward\",\n",
    "    )\n",
    "    model.fit(jnp.array(X), jnp.array(y))\n",
    "\n",
    "    X_plot = jnp.linspace(0, 5, 100).reshape(-1, 1)\n",
    "\n",
    "    # Fan chart\n",
    "    ax = plot_prediction_intervals(model, X_plot, y=jnp.array(y))\n",
    "    ax.set_title(\"Prediction Intervals Fan Chart\")\n",
    "    plt.savefig(\"uq_prediction_intervals.png\", dpi=100, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"\\n  Saved: uq_prediction_intervals.png\")\n",
    "\n",
    "    # Coefficient forest plot\n",
    "    ax = plot_coefficient_intervals(model)\n",
    "    plt.savefig(\"uq_coefficient_intervals.png\", dpi=100, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"  Saved: uq_coefficient_intervals.png\")\n",
    "\n",
    "    # BMA weights\n",
    "    ax = plot_bma_weights(model)\n",
    "    plt.savefig(\"uq_bma_weights.png\", dpi=100, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"  Saved: uq_bma_weights.png\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d270348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run all UQ examples.\"\"\"\n",
    "    print(\"JAXSR: Uncertainty Quantification Examples\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    example_classical_intervals()\n",
    "    example_ensemble_predictions()\n",
    "    example_bayesian_model_averaging()\n",
    "    example_conformal_prediction()\n",
    "    example_bootstrap()\n",
    "    example_visualization()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"All UQ examples completed!\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a88620",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "nbformat_minor": 5
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}