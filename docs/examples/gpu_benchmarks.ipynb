{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# JAXSR Performance: CPU vs GPU/MPS Benchmarking\n\nJAXSR uses JAX for its core linear algebra operations (`lstsq`, SVD, `pinv`, `matmul`),\nwhich are transparently accelerated on GPU when available. JAX dispatches these operations\nto device-specific kernels — cuBLAS/cuSOLVER on NVIDIA GPUs, Metal Performance Shaders on\nApple Silicon (via [jax-mps](https://github.com/jkitchin/jax-mps)), or MKL/OpenBLAS on CPU.\n\n**Key points:**\n- GPU/MPS advantage grows with problem size. Small problems may be faster on CPU due to kernel launch overhead.\n- Python-level loops (greedy selection iterations, basis function evaluation) run on CPU regardless;\n  the accelerator speeds up the individual JAX operations *within* those loops.\n- This notebook benchmarks 6 JAXSR features across varying problem sizes to show when acceleration matters.\n\nIf no GPU or MPS device is available, the notebook still runs and reports CPU-only timings."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\nfrom jaxsr import (\n    BasisLibrary,\n    SymbolicRegressor,\n    cross_validate,\n    bootstrap_model_selection,\n    discover_dynamics,\n)\n\n# --- Device detection ---\ncpu_device = jax.devices(\"cpu\")[0]\n\n# Try MPS (Apple Silicon) first, then CUDA GPU\ngpu_device = None\ngpu_platform = None\nfor platform_name in (\"mps\", \"gpu\"):\n    try:\n        gpu_device = jax.devices(platform_name)[0]\n        gpu_platform = platform_name\n        break\n    except (RuntimeError, ValueError):\n        continue\n\nHAS_GPU = gpu_device is not None\n\n\n# --- Benchmark utility ---\ndef benchmark(fn, device, warmup=1, repeats=5):\n    \"\"\"Time a function on the given JAX device.\n\n    Runs `warmup` calls to trigger JIT compilation, then times `repeats` runs\n    and returns the median wall-clock time in seconds.\n    \"\"\"\n    with jax.default_device(device):\n        # Warmup (JIT compilation)\n        for _ in range(warmup):\n            fn()\n            jnp.zeros(1).block_until_ready()\n\n        # Timed runs\n        times = []\n        for _ in range(repeats):\n            start = time.perf_counter()\n            fn()\n            jnp.zeros(1).block_until_ready()\n            elapsed = time.perf_counter() - start\n            times.append(elapsed)\n\n    return np.median(times)\n\n\n# --- Results collector ---\nresults = []"
  },
  {
   "cell_type": "code",
   "source": "import os\nimport platform\nimport subprocess\n\nprint(\"System Information\")\nprint(\"=\" * 60)\n\n# OS info\nprint(f\"OS:           {platform.system()} {platform.release()}\")\nprint(f\"Platform:     {platform.platform()}\")\nprint(f\"Python:       {platform.python_version()}\")\n\n# CPU info\nprint(f\"\\nCPU:          {platform.processor() or 'unknown'}\")\ncpu_count_physical = os.cpu_count()\nprint(f\"CPU cores:    {cpu_count_physical}\")\ntry:\n    with open(\"/proc/cpuinfo\") as f:\n        for line in f:\n            if line.startswith(\"model name\"):\n                print(f\"CPU model:    {line.split(':')[1].strip()}\")\n                break\nexcept FileNotFoundError:\n    pass\n\n# Memory info\ntry:\n    with open(\"/proc/meminfo\") as f:\n        for line in f:\n            if line.startswith(\"MemTotal\"):\n                mem_kb = int(line.split()[1])\n                print(f\"\\nMemory:       {mem_kb / 1024 / 1024:.1f} GB\")\n                break\nexcept FileNotFoundError:\n    pass\n\n# JAX info\nprint(f\"\\nJAX version:  {jax.__version__}\")\nprint(f\"JAX backend:  {jax.default_backend()}\")\nprint(f\"CPU device:   {cpu_device}\")\n\nif HAS_GPU:\n    print(f\"Accel device: {gpu_device} (platform: {gpu_platform})\")\n    if gpu_platform == \"mps\":\n        # Apple Silicon — report chip info via system_profiler\n        try:\n            sp_out = subprocess.check_output(\n                [\"sysctl\", \"-n\", \"machdep.cpu.brand_string\"],\n                text=True,\n            ).strip()\n            print(f\"Apple chip:   {sp_out}\")\n        except (FileNotFoundError, subprocess.CalledProcessError):\n            pass\n        try:\n            mem_bytes = int(subprocess.check_output(\n                [\"sysctl\", \"-n\", \"hw.memsize\"], text=True\n            ).strip())\n            print(f\"Unified mem:  {mem_bytes / 1024**3:.0f} GB\")\n        except (FileNotFoundError, subprocess.CalledProcessError, ValueError):\n            pass\n    else:\n        # NVIDIA GPU — report via nvidia-smi\n        try:\n            nvidia_out = subprocess.check_output(\n                [\"nvidia-smi\", \"--query-gpu=name,memory.total,driver_version\",\n                 \"--format=csv,noheader\"],\n                text=True,\n            ).strip()\n            for line in nvidia_out.split(\"\\n\"):\n                parts = [p.strip() for p in line.split(\",\")]\n                if len(parts) >= 3:\n                    print(f\"GPU model:    {parts[0]}\")\n                    print(f\"GPU memory:   {parts[1]}\")\n                    print(f\"NVIDIA driver:{parts[2]}\")\n        except (FileNotFoundError, subprocess.CalledProcessError):\n            print(\"GPU details:  nvidia-smi not available\")\nelse:\n    print(\"Accel device: Not available (CPU only)\")\n\nprint(f\"\\nNumPy:        {np.__version__}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 1: Basis Library Evaluation\n",
    "\n",
    "**What:** `BasisLibrary.evaluate(X)` constructs the design matrix $\\Phi$ by evaluating\n",
    "each basis function on the input data.\n",
    "\n",
    "**Why it matters:** This is the first step in every JAXSR workflow. The `evaluate()` call\n",
    "loops over each basis function in Python and calls `jnp.column_stack()`. Each elementwise\n",
    "op (e.g., `jnp.log`, `jnp.exp`, `x**3`) runs on the device, so GPU wins when `n_samples`\n",
    "is large enough to amortize kernel launch overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Benchmark 1: Basis Library Evaluation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Build a large library with 5 features\n",
    "library = (\n",
    "    BasisLibrary(n_features=5)\n",
    "    .add_constant()\n",
    "    .add_linear()\n",
    "    .add_polynomials(max_degree=4)\n",
    "    .add_interactions(max_order=3)\n",
    "    .add_transcendental([\"log\", \"exp\", \"sqrt\", \"inv\"])\n",
    ")\n",
    "print(f\"Library size: {len(library.names)} basis functions\")\n",
    "\n",
    "sizes = [1_000, 10_000, 100_000]\n",
    "\n",
    "for n in sizes:\n",
    "    rng = np.random.default_rng(42)\n",
    "    # Positive values needed for log/sqrt/inv\n",
    "    X_np = rng.uniform(0.1, 5.0, size=(n, 5))\n",
    "\n",
    "    cpu_time = benchmark(lambda: library.evaluate(X_np), cpu_device, warmup=1, repeats=5)\n",
    "    gpu_time = benchmark(lambda: library.evaluate(X_np), gpu_device, warmup=1, repeats=5) if HAS_GPU else None\n",
    "\n",
    "    speedup = cpu_time / gpu_time if gpu_time else None\n",
    "    gpu_str = f\"{gpu_time:.4f}s\" if gpu_time else \"N/A\"\n",
    "    sp_str = f\"{speedup:.2f}x\" if speedup else \"N/A\"\n",
    "    print(f\"  n={n:>7,}: CPU={cpu_time:.4f}s  GPU={gpu_str}  Speedup={sp_str}\")\n",
    "\n",
    "    results.append({\n",
    "        \"benchmark\": \"Basis Evaluation\",\n",
    "        \"size\": n,\n",
    "        \"cpu\": cpu_time,\n",
    "        \"gpu\": gpu_time,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 2: Model Fitting — Greedy Forward Selection\n",
    "\n",
    "**What:** `SymbolicRegressor.fit()` with `strategy=\"greedy_forward\"` iteratively adds\n",
    "basis functions that most improve the fit. Each iteration evaluates all remaining candidates\n",
    "via `lstsq` calls.\n",
    "\n",
    "**Why it matters:** This is the primary fitting workflow. With ~50 basis functions and\n",
    "`max_terms=8`, greedy forward evaluates hundreds of `lstsq` calls. At large `n_samples`,\n",
    "the GPU BLAS kernel for `lstsq` should clearly outperform CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Benchmark 2: Greedy Forward Selection\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "sizes = [500, 5_000, 50_000]\n",
    "\n",
    "for n in sizes:\n",
    "    rng = np.random.default_rng(42)\n",
    "    X_np = rng.uniform(0.1, 5.0, size=(n, 4))\n",
    "    x0, x1, x2, x3 = X_np[:, 0], X_np[:, 1], X_np[:, 2], X_np[:, 3]\n",
    "    y_np = 2.0 * x0 + 1.5 * x1**2 - 0.8 * x2 * x3 + 0.3 + rng.normal(0, 0.1, n)\n",
    "\n",
    "    lib = (\n",
    "        BasisLibrary(n_features=4)\n",
    "        .add_constant()\n",
    "        .add_linear()\n",
    "        .add_polynomials(max_degree=3)\n",
    "        .add_interactions(max_order=2)\n",
    "        .add_transcendental([\"log\", \"exp\", \"sqrt\", \"inv\"])\n",
    "    )\n",
    "\n",
    "    def run_greedy():\n",
    "        model = SymbolicRegressor(\n",
    "            basis_library=lib, max_terms=8, strategy=\"greedy_forward\",\n",
    "        )\n",
    "        model.fit(X_np, y_np)\n",
    "\n",
    "    cpu_time = benchmark(run_greedy, cpu_device, warmup=1, repeats=3)\n",
    "    gpu_time = benchmark(run_greedy, gpu_device, warmup=1, repeats=3) if HAS_GPU else None\n",
    "\n",
    "    speedup = cpu_time / gpu_time if gpu_time else None\n",
    "    gpu_str = f\"{gpu_time:.4f}s\" if gpu_time else \"N/A\"\n",
    "    sp_str = f\"{speedup:.2f}x\" if speedup else \"N/A\"\n",
    "    print(f\"  n={n:>7,}: CPU={cpu_time:.4f}s  GPU={gpu_str}  Speedup={sp_str}\")\n",
    "\n",
    "    results.append({\n",
    "        \"benchmark\": \"Greedy Forward\",\n",
    "        \"size\": n,\n",
    "        \"cpu\": cpu_time,\n",
    "        \"gpu\": gpu_time,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 3: Exhaustive Model Search\n",
    "\n",
    "**What:** `SymbolicRegressor.fit()` with `strategy=\"exhaustive\"` evaluates all subsets\n",
    "$\\binom{B}{k}$ for $k = 1, \\ldots, \\text{max\\_terms}$.\n",
    "\n",
    "**Why it matters:** This is the most computation-dense benchmark. With 10 basis functions\n",
    "and `max_terms=5`, there are $\\binom{10}{1} + \\cdots + \\binom{10}{5} = 637$ `lstsq` calls.\n",
    "Pure computation with minimal Python overhead between calls — best case for GPU advantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Benchmark 3: Exhaustive Model Search\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "sizes = [1_000, 10_000, 100_000]\n",
    "\n",
    "for n in sizes:\n",
    "    rng = np.random.default_rng(42)\n",
    "    X_np = rng.uniform(0.1, 5.0, size=(n, 2))\n",
    "    x0, x1 = X_np[:, 0], X_np[:, 1]\n",
    "    y_np = 3.0 * x0**2 - 1.5 * x0 * x1 + 0.5 + rng.normal(0, 0.1, n)\n",
    "\n",
    "    lib = (\n",
    "        BasisLibrary(n_features=2)\n",
    "        .add_constant()\n",
    "        .add_linear()\n",
    "        .add_polynomials(max_degree=3)\n",
    "        .add_interactions(max_order=2)\n",
    "    )\n",
    "    print(f\"  Library size: {len(lib.names)} basis functions\")\n",
    "\n",
    "    def run_exhaustive():\n",
    "        model = SymbolicRegressor(\n",
    "            basis_library=lib, max_terms=5, strategy=\"exhaustive\",\n",
    "        )\n",
    "        model.fit(X_np, y_np)\n",
    "\n",
    "    cpu_time = benchmark(run_exhaustive, cpu_device, warmup=1, repeats=3)\n",
    "    gpu_time = benchmark(run_exhaustive, gpu_device, warmup=1, repeats=3) if HAS_GPU else None\n",
    "\n",
    "    speedup = cpu_time / gpu_time if gpu_time else None\n",
    "    gpu_str = f\"{gpu_time:.4f}s\" if gpu_time else \"N/A\"\n",
    "    sp_str = f\"{speedup:.2f}x\" if speedup else \"N/A\"\n",
    "    print(f\"  n={n:>7,}: CPU={cpu_time:.4f}s  GPU={gpu_str}  Speedup={sp_str}\")\n",
    "\n",
    "    results.append({\n",
    "        \"benchmark\": \"Exhaustive Search\",\n",
    "        \"size\": n,\n",
    "        \"cpu\": cpu_time,\n",
    "        \"gpu\": gpu_time,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 4: Cross-Validation\n",
    "\n",
    "**What:** `cross_validate(model, X, y, cv=10)` performs 10-fold cross-validation.\n",
    "Each fold clones the model and does a full `fit()` on ~90% of the data.\n",
    "\n",
    "**Why it matters:** 10 independent model fits multiply the GPU advantage from\n",
    "Benchmark 2 by approximately 10x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Benchmark 4: Cross-Validation (10-fold)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "sizes = [1_000, 10_000, 50_000]\n",
    "\n",
    "for n in sizes:\n",
    "    rng = np.random.default_rng(42)\n",
    "    X_np = rng.uniform(0.1, 5.0, size=(n, 4))\n",
    "    x0, x1, x2, x3 = X_np[:, 0], X_np[:, 1], X_np[:, 2], X_np[:, 3]\n",
    "    y_np = 2.0 * x0 + 1.5 * x1**2 - 0.8 * x2 * x3 + 0.3 + rng.normal(0, 0.1, n)\n",
    "\n",
    "    lib = (\n",
    "        BasisLibrary(n_features=4)\n",
    "        .add_constant()\n",
    "        .add_linear()\n",
    "        .add_polynomials(max_degree=3)\n",
    "        .add_interactions(max_order=2)\n",
    "    )\n",
    "\n",
    "    model = SymbolicRegressor(\n",
    "        basis_library=lib, max_terms=8, strategy=\"greedy_forward\",\n",
    "    )\n",
    "\n",
    "    def run_cv():\n",
    "        cross_validate(model, X_np, y_np, cv=10, random_state=42)\n",
    "\n",
    "    cpu_time = benchmark(run_cv, cpu_device, warmup=0, repeats=3)\n",
    "    gpu_time = benchmark(run_cv, gpu_device, warmup=0, repeats=3) if HAS_GPU else None\n",
    "\n",
    "    speedup = cpu_time / gpu_time if gpu_time else None\n",
    "    gpu_str = f\"{gpu_time:.4f}s\" if gpu_time else \"N/A\"\n",
    "    sp_str = f\"{speedup:.2f}x\" if speedup else \"N/A\"\n",
    "    print(f\"  n={n:>7,}: CPU={cpu_time:.4f}s  GPU={gpu_str}  Speedup={sp_str}\")\n",
    "\n",
    "    results.append({\n",
    "        \"benchmark\": \"Cross-Validation\",\n",
    "        \"size\": n,\n",
    "        \"cpu\": cpu_time,\n",
    "        \"gpu\": gpu_time,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 5: Bootstrap Model Stability\n",
    "\n",
    "**What:** `bootstrap_model_selection(model, X, y, n_bootstrap=N)` resamples the data\n",
    "N times and refits the model each time to assess selection stability.\n",
    "\n",
    "**Why it matters:** Each bootstrap iteration clones the model and calls `fit()` on\n",
    "a resampled dataset. Similar to cross-validation but with more iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Benchmark 5: Bootstrap Model Stability\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "n = 2_000\n",
    "rng = np.random.default_rng(42)\n",
    "X_np = rng.uniform(0.1, 5.0, size=(n, 4))\n",
    "x0, x1, x2, x3 = X_np[:, 0], X_np[:, 1], X_np[:, 2], X_np[:, 3]\n",
    "y_np = 2.0 * x0 + 1.5 * x1**2 - 0.8 * x2 * x3 + 0.3 + rng.normal(0, 0.1, n)\n",
    "\n",
    "lib = (\n",
    "    BasisLibrary(n_features=4)\n",
    "    .add_constant()\n",
    "    .add_linear()\n",
    "    .add_polynomials(max_degree=3)\n",
    "    .add_interactions(max_order=2)\n",
    ")\n",
    "\n",
    "model = SymbolicRegressor(\n",
    "    basis_library=lib, max_terms=8, strategy=\"greedy_forward\",\n",
    ")\n",
    "# Fit once so bootstrap_model_selection can clone from a fitted model\n",
    "with jax.default_device(cpu_device):\n",
    "    model.fit(X_np, y_np)\n",
    "\n",
    "bootstrap_sizes = [20, 50]\n",
    "\n",
    "for n_boot in bootstrap_sizes:\n",
    "    def run_bootstrap():\n",
    "        bootstrap_model_selection(model, X_np, y_np, n_bootstrap=n_boot, seed=42)\n",
    "\n",
    "    cpu_time = benchmark(run_bootstrap, cpu_device, warmup=0, repeats=3)\n",
    "    gpu_time = benchmark(run_bootstrap, gpu_device, warmup=0, repeats=3) if HAS_GPU else None\n",
    "\n",
    "    speedup = cpu_time / gpu_time if gpu_time else None\n",
    "    gpu_str = f\"{gpu_time:.4f}s\" if gpu_time else \"N/A\"\n",
    "    sp_str = f\"{speedup:.2f}x\" if speedup else \"N/A\"\n",
    "    print(f\"  n_bootstrap={n_boot:>3}: CPU={cpu_time:.4f}s  GPU={gpu_str}  Speedup={sp_str}\")\n",
    "\n",
    "    results.append({\n",
    "        \"benchmark\": \"Bootstrap Stability\",\n",
    "        \"size\": n_boot,\n",
    "        \"cpu\": cpu_time,\n",
    "        \"gpu\": gpu_time,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 6: ODE/Dynamics Discovery\n",
    "\n",
    "**What:** `discover_dynamics(X, t, ...)` estimates derivatives from time-series data,\n",
    "then fits one `SymbolicRegressor` per state variable.\n",
    "\n",
    "**Setup:** Lotka-Volterra predator-prey system:\n",
    "$$\\frac{dx}{dt} = \\alpha x - \\beta xy, \\quad \\frac{dy}{dt} = \\delta xy - \\gamma y$$\n",
    "\n",
    "**Why it matters:** Mixed workload — derivative estimation uses NumPy/SciPy (always CPU),\n",
    "but the symbolic regression fits use JAX. Shows a realistic scientific workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Benchmark 6: ODE/Dynamics Discovery\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Lotka-Volterra parameters\n",
    "alpha, beta, delta, gamma = 1.0, 0.1, 0.075, 1.5\n",
    "\n",
    "\n",
    "def lotka_volterra(t, z):\n",
    "    x, y = z\n",
    "    return [alpha * x - beta * x * y, delta * x * y - gamma * y]\n",
    "\n",
    "\n",
    "sizes = [500, 5_000, 50_000]\n",
    "\n",
    "for n_pts in sizes:\n",
    "    t_span = (0.0, 15.0)\n",
    "    t_eval = np.linspace(*t_span, n_pts)\n",
    "    sol = solve_ivp(lotka_volterra, t_span, [10.0, 5.0], t_eval=t_eval, method=\"RK45\")\n",
    "    X_dyn = sol.y.T  # shape (n_pts, 2)\n",
    "    t_arr = sol.t\n",
    "\n",
    "    def run_dynamics():\n",
    "        discover_dynamics(\n",
    "            X_dyn, t_arr,\n",
    "            state_names=[\"prey\", \"predator\"],\n",
    "            max_terms=5,\n",
    "            strategy=\"greedy_forward\",\n",
    "        )\n",
    "\n",
    "    cpu_time = benchmark(run_dynamics, cpu_device, warmup=0, repeats=3)\n",
    "    gpu_time = benchmark(run_dynamics, gpu_device, warmup=0, repeats=3) if HAS_GPU else None\n",
    "\n",
    "    speedup = cpu_time / gpu_time if gpu_time else None\n",
    "    gpu_str = f\"{gpu_time:.4f}s\" if gpu_time else \"N/A\"\n",
    "    sp_str = f\"{speedup:.2f}x\" if speedup else \"N/A\"\n",
    "    print(f\"  n_pts={n_pts:>7,}: CPU={cpu_time:.4f}s  GPU={gpu_str}  Speedup={sp_str}\")\n",
    "\n",
    "    results.append({\n",
    "        \"benchmark\": \"ODE Discovery\",\n",
    "        \"size\": n_pts,\n",
    "        \"cpu\": cpu_time,\n",
    "        \"gpu\": gpu_time,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Summary Table ---\naccel_label = gpu_platform.upper() if gpu_platform else \"GPU\"\n\nprint(\"\\nPerformance Summary\")\nprint(\"=\" * 75)\nheader = f\"{'Benchmark':<22} {'Size':>10} {'CPU (s)':>10} {accel_label + ' (s)':>10} {'Speedup':>10}\"\nprint(header)\nprint(\"-\" * 75)\nfor r in results:\n    gpu_str = f\"{r['gpu']:.4f}\" if r[\"gpu\"] is not None else \"N/A\"\n    speedup = r[\"cpu\"] / r[\"gpu\"] if r[\"gpu\"] else None\n    sp_str = f\"{speedup:.2f}x\" if speedup else \"N/A\"\n    print(f\"{r['benchmark']:<22} {r['size']:>10,} {r['cpu']:>10.4f} {gpu_str:>10} {sp_str:>10}\")\n\n# --- Visualization ---\n# Use the largest problem size for each benchmark\nbenchmarks_seen = []\nlargest = {}\nfor r in results:\n    name = r[\"benchmark\"]\n    if name not in largest or r[\"size\"] > largest[name][\"size\"]:\n        largest[name] = r\n    if name not in benchmarks_seen:\n        benchmarks_seen.append(name)\n\nbench_names = benchmarks_seen\ncpu_times = [largest[b][\"cpu\"] for b in bench_names]\ngpu_times = [largest[b][\"gpu\"] for b in bench_names]\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot 1: CPU vs GPU/MPS bar chart\nax = axes[0]\nx_pos = np.arange(len(bench_names))\nbar_width = 0.35\nax.bar(x_pos - bar_width / 2, cpu_times, bar_width, label=\"CPU\", color=\"steelblue\")\nif HAS_GPU:\n    gpu_vals = [g if g is not None else 0 for g in gpu_times]\n    ax.bar(x_pos + bar_width / 2, gpu_vals, bar_width, label=accel_label, color=\"coral\")\nax.set_yscale(\"log\")\nax.set_ylabel(\"Time (s, log scale)\")\nax.set_title(f\"CPU vs {accel_label} — Largest Problem Size\")\nax.set_xticks(x_pos)\nax.set_xticklabels(bench_names, rotation=30, ha=\"right\", fontsize=8)\nax.legend()\nax.grid(axis=\"y\", alpha=0.3)\n\n# Plot 2: Speedup bar chart\nax = axes[1]\nif HAS_GPU:\n    speedups = [\n        largest[b][\"cpu\"] / largest[b][\"gpu\"]\n        if largest[b][\"gpu\"] is not None\n        else 0\n        for b in bench_names\n    ]\n    colors = [\"seagreen\" if s > 1 else \"indianred\" for s in speedups]\n    ax.barh(bench_names, speedups, color=colors)\n    ax.axvline(x=1.0, color=\"black\", linestyle=\"--\", linewidth=1, label=\"Break-even\")\n    ax.set_xlabel(f\"Speedup (CPU time / {accel_label} time)\")\n    ax.set_title(f\"{accel_label} Speedup — Largest Problem Size\")\n    ax.legend()\n    ax.grid(axis=\"x\", alpha=0.3)\nelse:\n    ax.text(\n        0.5, 0.5, \"No accelerator available\\nSpeedup chart requires GPU or MPS\",\n        ha=\"center\", va=\"center\", transform=ax.transAxes, fontsize=12,\n    )\n    ax.set_title(\"Accelerator Speedup — N/A\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways\n\n1. **Accelerator overhead dominates for small problems.** JAXSR's core workflow involves many\n   small `lstsq` calls inside Python loops (greedy selection, exhaustive search). Each\n   GPU/MPS kernel launch has fixed overhead (~0.1–1 ms), and when the matrices are small,\n   this overhead exceeds the computation time. CPU avoids this overhead entirely.\n\n2. **Accelerators only help at very large `n_samples`.** The crossover point where\n   GPU/MPS matches CPU is roughly 50K–100K samples for most workflows.\n\n3. **Python-level loops are the real bottleneck (Amdahl's law).** Greedy forward selection\n   iterates in Python over candidate basis functions. Even with instant linear algebra,\n   the loop overhead caps speedup.\n\n4. **Basis evaluation benefits most.** This is the most \"accelerator-friendly\" operation: each\n   basis function is an elementwise op on a large array, with minimal Python loop overhead\n   relative to computation.\n\n5. **For typical JAXSR workloads, CPU is faster.** Unless you are fitting models with\n   >50K samples, stick with CPU. Set `JAX_PLATFORMS=cpu` to avoid kernel launch overhead:\n   ```python\n   import os\n   os.environ[\"JAX_PLATFORMS\"] = \"cpu\"\n   ```\n\n6. **Where accelerators *would* help.** If JAXSR's inner loops were replaced with\n   batched/vmapped JAX operations (e.g., vmapping lstsq over all candidate subsets at once),\n   the GPU/MPS advantage would be dramatic. This is a potential future optimization.\n\n7. **Vectorized bootstrap functions are already efficient.** `bootstrap_coefficients()` and\n   `bootstrap_predict()` compute the pseudo-inverse once and apply it to all bootstrap\n   samples in a single matmul — so the per-iteration cost is negligible regardless of device."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}