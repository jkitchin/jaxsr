{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc2a524",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Active Learning & Acquisition Functions Example for JAXSR.\n",
    "\n",
    "Demonstrates how to use acquisition functions to intelligently select\n",
    "the next experiments to run, balancing exploration and exploitation.\n",
    "\n",
    "This example covers five scenarios:\n",
    "\n",
    "1. Pure exploration: reduce uncertainty everywhere\n",
    "2. Bayesian optimisation: find the minimum of an unknown function\n",
    "3. Model discrimination: resolve which model structure is correct\n",
    "4. Batch selection strategies: greedy vs penalized vs kriging believer\n",
    "5. Full active learning loop: iteratively improve a model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8581c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edda0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxsr import BasisLibrary, SymbolicRegressor\n",
    "from jaxsr.acquisition import (\n",
    "    LCB,\n",
    "    ActiveLearner,\n",
    "    AOptimal,\n",
    "    BMAUncertainty,\n",
    "    ConfidenceBandWidth,\n",
    "    DOptimal,\n",
    "    EnsembleDisagreement,\n",
    "    ExpectedImprovement,\n",
    "    ModelDiscrimination,\n",
    "    ModelMin,\n",
    "    PredictionVariance,\n",
    "    ProbabilityOfImprovement,\n",
    "    ThompsonSampling,\n",
    "    suggest_points,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e68ce9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "=========================================================================\n",
    "Shared setup: fit a model we'll use across examples\n",
    "========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef5e56c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    \"\"\"Create a fitted model on synthetic data: y = x^2 - 3x + 2 + noise.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    X = np.random.uniform(0, 5, (40, 1))\n",
    "    y = X[:, 0] ** 2 - 3.0 * X[:, 0] + 2.0 + np.random.randn(40) * 0.3\n",
    "\n",
    "    library = (\n",
    "        BasisLibrary(n_features=1, feature_names=[\"x\"])\n",
    "        .add_constant()\n",
    "        .add_linear()\n",
    "        .add_polynomials(max_degree=3)\n",
    "    )\n",
    "    model = SymbolicRegressor(basis_library=library, max_terms=4, strategy=\"greedy_forward\")\n",
    "    model.fit(jnp.array(X), jnp.array(y))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c919dc07",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "=========================================================================\n",
    "Example 1: Pure Exploration - Reduce Uncertainty\n",
    "========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dd91e5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def example_exploration():\n",
    "    \"\"\"\n",
    "    Goal: improve model accuracy uniformly by sampling where\n",
    "    prediction uncertainty is highest.\n",
    "\n",
    "    Available acquisition functions for this goal:\n",
    "\n",
    "    - PredictionVariance: the default.  Uses the OLS posterior to compute\n",
    "      sigma^2(x).  Fast, exact for linear-in-parameter models.\n",
    "\n",
    "    - ConfidenceBandWidth: similar, but reports the actual width of the\n",
    "      confidence band at a specified significance level.\n",
    "\n",
    "    - EnsembleDisagreement: uses the Pareto front of models with different\n",
    "      complexities.  Good when you're unsure whether a simpler or more\n",
    "      complex model is appropriate.\n",
    "\n",
    "    - BMAUncertainty: Bayesian Model Averaging.  The most comprehensive\n",
    "      measure -- combines noise uncertainty and model-selection uncertainty.\n",
    "\n",
    "    - AOptimal: targets reduction in *parameter* uncertainty (trace of\n",
    "      covariance matrix).  Use when you care about accurate coefficients.\n",
    "\n",
    "    - DOptimal: maximises information gain (det of information matrix).\n",
    "      Use when you want maximal information per experiment.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Example 1: Pure Exploration\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    model = make_model()\n",
    "    bounds = [(0.0, 5.0)]\n",
    "\n",
    "    print(f\"Current model: {model.expression_}\")\n",
    "    print(f\"Current R^2:   {model.score(model._X_train, model._y_train):.4f}\")\n",
    "    print(f\"Training size: {len(model._y_train)}\")\n",
    "    print()\n",
    "\n",
    "    # --- Try different exploration strategies ---\n",
    "    strategies = [\n",
    "        (\"PredictionVariance\", PredictionVariance()),\n",
    "        (\"ConfidenceBandWidth(95%)\", ConfidenceBandWidth(alpha=0.05)),\n",
    "        (\"EnsembleDisagreement\", EnsembleDisagreement()),\n",
    "        (\"BMAUncertainty\", BMAUncertainty(criterion=\"bic\")),\n",
    "        (\"AOptimal\", AOptimal()),\n",
    "        (\"DOptimal\", DOptimal()),\n",
    "    ]\n",
    "\n",
    "    for name, acq in strategies:\n",
    "        result = suggest_points(model, bounds, acq, n_points=3, random_state=42)\n",
    "        pts = np.array(result.points).ravel()\n",
    "        print(f\"  {name:30s} -> x = [{', '.join(f'{p:.2f}' for p in pts)}]\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef35deb7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "=========================================================================\n",
    "Example 2: Bayesian Optimisation - Find the Minimum\n",
    "========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084d98ba",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def example_optimisation():\n",
    "    \"\"\"\n",
    "    Goal: find x that minimises y, using the fitted model as a surrogate.\n",
    "\n",
    "    Available acquisition functions for this goal:\n",
    "\n",
    "    - ModelMin / ModelMax: pure exploitation.  No exploration at all --\n",
    "      just returns the predicted optimum.  Use only when you fully trust\n",
    "      the model.\n",
    "\n",
    "    - LCB (Lower Confidence Bound): y_hat - kappa*sigma.  The kappa\n",
    "      parameter controls exploration vs exploitation:\n",
    "        kappa=0  -> pure exploitation (ModelMin)\n",
    "        kappa~2  -> balanced\n",
    "        kappa>3  -> heavy exploration\n",
    "\n",
    "    - UCB (Upper Confidence Bound): the mirror image for maximisation.\n",
    "\n",
    "    - ExpectedImprovement (EI): the Bayesian optimisation gold standard.\n",
    "      Naturally balances exploration and exploitation without a tuning\n",
    "      parameter (just xi, which is usually small).  Recommended as the\n",
    "      default for optimisation.\n",
    "\n",
    "    - ProbabilityOfImprovement (PI): similar to EI but only cares about\n",
    "      the *probability* of beating the current best, not the magnitude\n",
    "      of improvement.  More exploitative than EI for the same xi.\n",
    "\n",
    "    - ThompsonSampling: draws a random model from the posterior and\n",
    "      optimises that.  Produces diverse batches naturally.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Example 2: Bayesian Optimisation (Minimise y)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    model = make_model()\n",
    "    bounds = [(0.0, 5.0)]\n",
    "\n",
    "    print(f\"Model: {model.expression_}\")\n",
    "    print(\"True minimum at x=1.5 (y = 2.25 - 4.5 + 2 = -0.25)\")\n",
    "    print()\n",
    "\n",
    "    strategies = [\n",
    "        (\"ModelMin (exploit only)\", ModelMin()),\n",
    "        (\"LCB kappa=0.5 (exploitative)\", LCB(kappa=0.5)),\n",
    "        (\"LCB kappa=2 (balanced)\", LCB(kappa=2.0)),\n",
    "        (\"LCB kappa=5 (exploratory)\", LCB(kappa=5.0)),\n",
    "        (\"Expected Improvement\", ExpectedImprovement(minimize=True)),\n",
    "        (\"Prob. of Improvement\", ProbabilityOfImprovement(minimize=True)),\n",
    "        (\"Thompson Sampling\", ThompsonSampling(minimize=True, seed=42)),\n",
    "    ]\n",
    "\n",
    "    for name, acq in strategies:\n",
    "        result = suggest_points(model, bounds, acq, n_points=3, random_state=42)\n",
    "        pts = np.array(result.points).ravel()\n",
    "        print(f\"  {name:35s} -> x = [{', '.join(f'{p:.2f}' for p in pts)}]\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7145e4a8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "=========================================================================\n",
    "Example 3: Model Discrimination\n",
    "========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202dd7ef",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def example_discrimination():\n",
    "    \"\"\"\n",
    "    Goal: figure out which model form is correct.\n",
    "\n",
    "    When the Pareto front contains models of different complexities that\n",
    "    all fit the data similarly, you need data points that *discriminate*\n",
    "    between them.\n",
    "\n",
    "    - ModelDiscrimination: scores candidates by the maximum disagreement\n",
    "      among Pareto-front models.\n",
    "\n",
    "    - EnsembleDisagreement: standard deviation across Pareto models.\n",
    "      Similar idea but uses std instead of max-min range.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Example 3: Model Discrimination\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    model = make_model()\n",
    "    bounds = [(0.0, 5.0)]\n",
    "\n",
    "    print(f\"Best model: {model.expression_}\")\n",
    "    print(f\"Pareto front has {len(model.pareto_front_)} models:\")\n",
    "    for r in model.pareto_front_:\n",
    "        print(f\"  complexity={r.complexity}, BIC={r.bic:.1f}: {r.expression()}\")\n",
    "    print()\n",
    "\n",
    "    acqs = [\n",
    "        (\"ModelDiscrimination\", ModelDiscrimination()),\n",
    "        (\"EnsembleDisagreement\", EnsembleDisagreement()),\n",
    "    ]\n",
    "\n",
    "    for name, acq in acqs:\n",
    "        result = suggest_points(model, bounds, acq, n_points=5, random_state=42)\n",
    "        pts = np.array(result.points).ravel()\n",
    "        print(f\"  {name:25s} -> x = [{', '.join(f'{p:.2f}' for p in pts)}]\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a3f3c7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "=========================================================================\n",
    "Example 4: Batch Selection Strategies\n",
    "========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2181d6c2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def example_batch_strategies():\n",
    "    \"\"\"\n",
    "    Goal: select a *batch* of points that are collectively informative.\n",
    "\n",
    "    When you select the top-k by acquisition score (greedy), the points\n",
    "    can cluster in one region.  Batch strategies address this:\n",
    "\n",
    "    - greedy: top-k by raw score.  Fast but may cluster.\n",
    "\n",
    "    - penalized: after selecting the best candidate, nearby candidates\n",
    "      are penalised before selecting the next.  Simple diversity.\n",
    "\n",
    "    - kriging_believer: after selecting each point, the model is\n",
    "      temporarily updated with a \"fantasy\" observation (y_hat) and\n",
    "      re-scored.  More sophisticated -- later selections account for\n",
    "      information gained by earlier ones.\n",
    "\n",
    "    - d_optimal: selects the batch that maximises det(Phi^T Phi),\n",
    "      ignoring the acquisition function entirely.  Best for pure\n",
    "      space-filling / information maximisation.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Example 4: Batch Selection Strategies\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    model = make_model()\n",
    "    bounds = [(0.0, 5.0)]\n",
    "\n",
    "    learner = ActiveLearner(model, bounds, PredictionVariance(), random_state=42)\n",
    "\n",
    "    for strategy in [\"greedy\", \"penalized\", \"kriging_believer\", \"d_optimal\"]:\n",
    "        result = learner.suggest(n_points=5, batch_strategy=strategy)\n",
    "        pts = sorted(np.array(result.points).ravel())\n",
    "        spread = pts[-1] - pts[0]\n",
    "        print(\n",
    "            f\"  {strategy:20s} -> x = [{', '.join(f'{p:.2f}' for p in pts)}]\"\n",
    "            f\"  (spread={spread:.2f})\"\n",
    "        )\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae11415",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "=========================================================================\n",
    "Example 5: Full Active Learning Loop\n",
    "========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679a267c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def example_full_loop():\n",
    "    \"\"\"\n",
    "    Goal: iteratively improve a model by running experiments.\n",
    "\n",
    "    The workflow is:\n",
    "    1. Fit an initial model on a small dataset.\n",
    "    2. Use an acquisition function to suggest new points.\n",
    "    3. \"Run the experiment\" (here: evaluate the true function + noise).\n",
    "    4. Update the model with the new data.\n",
    "    5. Repeat until converged or budget exhausted.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Example 5: Full Active Learning Loop\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # True function (unknown to the model)\n",
    "    def oracle(X):\n",
    "        X = np.array(X)\n",
    "        return X[:, 0] ** 2 - 3.0 * X[:, 0] + 2.0 + np.random.randn(len(X)) * 0.2\n",
    "\n",
    "    # Start with very few points\n",
    "    np.random.seed(0)\n",
    "    X_init = np.random.uniform(0, 5, (15, 1))\n",
    "    y_init = oracle(X_init)\n",
    "\n",
    "    library = (\n",
    "        BasisLibrary(n_features=1, feature_names=[\"x\"])\n",
    "        .add_constant()\n",
    "        .add_linear()\n",
    "        .add_polynomials(max_degree=3)\n",
    "    )\n",
    "    model = SymbolicRegressor(basis_library=library, max_terms=4, strategy=\"greedy_forward\")\n",
    "    model.fit(jnp.array(X_init), jnp.array(y_init))\n",
    "\n",
    "    print(f\"Initial model ({len(y_init)} points): {model.expression_}\")\n",
    "    print(f\"  R^2 = {model.score(model._X_train, model._y_train):.4f}\")\n",
    "    print(f\"  MSE = {model.metrics_['mse']:.4f}\")\n",
    "\n",
    "    # Active learning loop\n",
    "    learner = ActiveLearner(\n",
    "        model,\n",
    "        bounds=[(0.0, 5.0)],\n",
    "        acquisition=ExpectedImprovement(minimize=True),\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    n_iterations = 5\n",
    "    points_per_iteration = 5\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        result = learner.suggest(\n",
    "            n_points=points_per_iteration,\n",
    "            batch_strategy=\"penalized\",\n",
    "        )\n",
    "\n",
    "        y_new = oracle(np.array(result.points))\n",
    "        learner.update(result.points, jnp.array(y_new))\n",
    "\n",
    "        print(\n",
    "            f\"  Iteration {i + 1}: \"\n",
    "            f\"n={learner.n_observations}, \"\n",
    "            f\"R^2={model.score(model._X_train, model._y_train):.4f}, \"\n",
    "            f\"MSE={model.metrics_['mse']:.4f}, \"\n",
    "            f\"model={model.expression_}\"\n",
    "        )\n",
    "\n",
    "    print(f\"\\nFinal model ({learner.n_observations} points): {model.expression_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bc6db6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "=========================================================================\n",
    "Example 6: Composite Acquisition Functions\n",
    "========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e96be8f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def example_composite():\n",
    "    \"\"\"\n",
    "    Goal: combine multiple objectives using weighted acquisition.\n",
    "\n",
    "    You can weight and add acquisition functions together to balance\n",
    "    different goals simultaneously.  Each component is min-max normalised\n",
    "    before weighting so the weights are meaningful.\n",
    "\n",
    "    Common recipes:\n",
    "    - Balanced optimisation:  0.7 * EI + 0.3 * PredictionVariance\n",
    "    - Exploration with model improvement:  0.5 * PredictionVariance + 0.5 * AOptimal\n",
    "    - Multi-objective:  0.4 * ModelMin + 0.3 * PredictionVariance + 0.3 * DOptimal\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Example 6: Composite Acquisition Functions\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    model = make_model()\n",
    "    bounds = [(0.0, 5.0)]\n",
    "\n",
    "    composites = [\n",
    "        (\n",
    "            \"0.7*EI + 0.3*Variance\",\n",
    "            0.7 * ExpectedImprovement(minimize=True) + 0.3 * PredictionVariance(),\n",
    "        ),\n",
    "        (\n",
    "            \"0.5*LCB + 0.5*DOptimal\",\n",
    "            0.5 * LCB(kappa=2) + 0.5 * DOptimal(),\n",
    "        ),\n",
    "        (\n",
    "            \"Equal: EI + Var + AOptimal\",\n",
    "            ExpectedImprovement(minimize=True) + PredictionVariance() + AOptimal(),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    for name, acq in composites:\n",
    "        result = suggest_points(model, bounds, acq, n_points=3, random_state=42)\n",
    "        pts = np.array(result.points).ravel()\n",
    "        print(f\"  {name:30s} -> x = [{', '.join(f'{p:.2f}' for p in pts)}]\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb1f0f1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "=========================================================================\n",
    "Decision Guide\n",
    "========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce42aa5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def print_decision_guide():\n",
    "    \"\"\"Print a guide for choosing among acquisition functions.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Decision Guide: Choosing an Acquisition Function\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    guide = \"\"\"\n",
    "    WHAT IS YOUR GOAL?\n",
    "\n",
    "    1. IMPROVE MODEL ACCURACY (explore everywhere)\n",
    "       \u251c\u2500\u2500 Simple & fast?  -> PredictionVariance\n",
    "       \u251c\u2500\u2500 Need coverage guarantee?  -> ConfidenceBandWidth(alpha=0.05)\n",
    "       \u251c\u2500\u2500 Unsure about model form?  -> EnsembleDisagreement or BMAUncertainty\n",
    "       \u251c\u2500\u2500 Tighten coefficient CIs?  -> AOptimal\n",
    "       \u2514\u2500\u2500 Max info per experiment?  -> DOptimal\n",
    "\n",
    "    2. FIND THE OPTIMUM (minimise or maximise y)\n",
    "       \u251c\u2500\u2500 Trust the model fully?  -> ModelMin / ModelMax\n",
    "       \u251c\u2500\u2500 Want balanced exploration?  -> ExpectedImprovement (recommended)\n",
    "       \u251c\u2500\u2500 Need probability of beating a threshold?  -> ProbabilityOfImprovement\n",
    "       \u251c\u2500\u2500 Want explicit exploration knob?  -> LCB(kappa) / UCB(kappa)\n",
    "       \u2514\u2500\u2500 Want randomised exploration?  -> ThompsonSampling\n",
    "\n",
    "    3. DECIDE WHICH MODEL IS CORRECT\n",
    "       \u251c\u2500\u2500 Pareto models disagree?  -> ModelDiscrimination\n",
    "       \u2514\u2500\u2500 Quantify structural uncertainty?  -> EnsembleDisagreement\n",
    "\n",
    "    4. MULTIPLE OBJECTIVES\n",
    "       \u2514\u2500\u2500 Combine with weights:  0.7 * EI + 0.3 * PredictionVariance\n",
    "\n",
    "    BATCH STRATEGY SELECTION:\n",
    "       \u251c\u2500\u2500 Fast, don't care about diversity?  -> greedy\n",
    "       \u251c\u2500\u2500 Want spatial diversity?  -> penalized\n",
    "       \u251c\u2500\u2500 Want information-aware batches?  -> kriging_believer\n",
    "       \u2514\u2500\u2500 Want maximum design efficiency?  -> d_optimal\n",
    "    \"\"\"\n",
    "    print(guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50993401",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "=========================================================================\n",
    "Main\n",
    "========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run all active learning examples.\"\"\"\n",
    "    print(\"JAXSR: Active Learning & Acquisition Functions\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "\n",
    "    example_exploration()\n",
    "    example_optimisation()\n",
    "    example_discrimination()\n",
    "    example_batch_strategies()\n",
    "    example_full_loop()\n",
    "    example_composite()\n",
    "    print_decision_guide()\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"All examples completed successfully!\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33fad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "nbformat_minor": 5
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}